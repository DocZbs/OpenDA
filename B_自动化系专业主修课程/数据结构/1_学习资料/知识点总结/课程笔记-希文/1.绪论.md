## 计算

计算 = 信息处理 = 借助某种工具，遵照一定规则，以明确而机械的形式进行  

计算模型 = 计算机 = 信息处理工具  

所谓算法，即特定计算模型下，旨在解决特定问题的指令序列。其特点为确定性（任一算法都可以描述为一个由基本操作组成的序列  ）、可行性（每一基本操作都可实现，且在常数时间内完成  ）、有穷性（对于任何输入，经有穷次基本操作，都可以得到输出  ）

### 有穷性

考察Hailstone序列
$$
\text { Hailstone }(n)= \begin{cases}\{1\} & (n \leq 1) \\ \{n\} \cup \text { Hailstone }(n / 2) & (n \text { is even }) \\ \{n\} \cup \text { Hailstone }(3 n+1) & (n \text { is odd })\end{cases}
$$

$$
\text{Hailstone}(42) = \{42,21,64,32,...,1\}
$$

 

可以写一个计算hailstone序列长度的算法：

```C++
int hailstone(int n){
    int length = 1;
    while(1 < n){
        (n % 2) ? n = 3 * n +1: n /= 2;
        length++;
        return length;
    }
}
```

这个算法是有穷的吗？即：对于任意的n，总有$|Hailstone(n)|<\infty $吗？

这个问题目前还没能在数学上得到解决。

### 好算法

评价一个算法是否“好”，我们最强调的方面是——效率。

速度尽可能快；存储空间尽可能少。
$$
Algorithms + Data Structures = Programs
$$

$$
(Algorithms + Data Structures) \times Efficiency = Computation
$$



## 计算模型

### 性能测度

算法分析：

- 正确：算法功能与问题要求一致？

- 成本：运行时间+所需存储空间（更关注时间成本）

如何度量？如何比较？

如果，将计算成本描述为函数，比如：
$$
  T_A(P) = 算法A求解问题实例P的计算成本
$$
但是这样的方式意义不大，因为可能出现的问题实例太多。

观察：问题实例的规模，往往是决定计算成本的最主要因素。（虽然也能找到反例，但定性的、大体上是这样的）

通常：规模接近，计算成本也接近；规模扩大，计算成本亦上升。

所以，令：
$$
  T_A(n) = 用算法A求解某一问题规模为n的实例时所需的计算成本
$$

然而这一定义仍有问题：同一问题规模的不同实例，计算成本不尽相同；有些场合，甚至会有实质性差别——“运气”好、坏。

既然如此，应在规模同为n的所有实例中，只关注最坏（成本最高）者：
$$
T(n) = max\{T(P) | \abs{p} = n\}
$$
需要抽象出一个理想模型，来衡量算法。

### 图灵机

图灵机有如下元素组成：

- Tape：以此均匀地划分为单元格，每个单元格各存有某一字符， 初始均为'#'

  字符来源于Alphabet，所有字符地种类是有限的。

- Head:总是对准某一单元格，并可读取或改写其中的字符。每经过一个节拍，可转向左侧或右侧的邻格。

  >  State: 
  >
  > -  图灵机总是处于有限种状态中的某一种  
  > - 每经过一个节拍可按照规则转向另一种状态  
  > - 统一约定， 'h' = halt  

  > Transition Function:(q, c; d, L/R, p)
  >
  > 转移规则由5个因素决定：若当前状态为q且当前字符为c，则将当前字符改为d,之后向左或向右移动到邻格，并将自己的状态改为p。
  >
  > 特别地，一旦转入约定的状态'h'，则停机  。
  >
  > 从启动至停机，所经历的节拍数目，亦等于Head累计的移动次数（无量纲） ，即可用以度量计算的成本  。   

### RAM模型

random access machine:在可计算的角度来讲，和图灵机类似。

随机存取存储器（英语：Random Access Memory，缩写：RAM），也叫主存，是与CPU直接交换数据的内部存储器。它可以随时读写（刷新时除外），而且速度很快，通常作为操作系统或其他正在运行中的程序的临时数据存储介质。RAM工作时可以随时从任何一个指定的地址写入（存入）或读出（取出）信息。它与ROM的最大区别是数据的易失性，即一旦断电所存储的数据将随之丢失。RAM在计算机和数字系统中用来暂时存储程序、数据和中间结果。

与TM模型一样， RAM模型也是一般计算工具的简化与抽象，使我们可以独立于具体的平台，对算法的效率做出可信的比较与评判 。

RAM模型中寄存器的总数没有限制（虽然我们平时使用的计算机无法做到），它与图灵机是等价的。

在这些模型中：

- 算法的运行时间 正比于 算法需要执行的基本操作次数  
- T(n) = 算法为求解规模为n的问题，所需执行的基本操作次数  

时间成本 ~ 各条指令执行次数之总和  

## 数据结构

相互之间存在一种或多种特定关系的数据元素的集合。

数据结构描述的是按照一定逻辑关系组织起来的待处理数据元素的表示及相关操作，涉及数据的逻辑结构、存储结构和数据的运算。

数据结构三元素：逻辑结构、存储结构、数据的运算

- 数据元素间的逻辑关系，即数据的逻辑结构；

  - Data_structure=(D,R)

    D: 数据元素的有限集合；  R: D上关系的有限集合

  - 四种数据逻辑结构：集合结构、线性结构、树形结构、图形结构。线性表、栈、队列为线性结构，树和图都是非线性结构。

    现在，另数据结构的二元组形式为：DS = (D, S)，则：

    1. 如果 D != null，而S == null，则该数据结构为集合结构。

    2. 如果 D = {01, 02, 03, 04, 05}，S = {<02,04>, <03,05>, <05,02>, <01,03>}，则该数据结构是线性结构。
       在这些数据元素中有一个可以被称为“第一个”的数据元素；还有一个可以被称为“最后一个”的数据元素；除第一个元素以外每个数据元素有且仅有一个直接前驱元素，除最后一个元素以外每个数据元素有且仅有一个直接后续元素。这种数据结构的特点是数据元素之间是 1对 1 的联系，即线性关系。

    3. D = {01, 02, 03, 04, 05, 06}
       S = {<01,02>, <01,03>, <02,04>, <02,05>, <03,06>}
       除了一个数据元素（元素 01）以外每个数据元素有且仅有一个直接前驱元素，但是可以有多个直接后续元素。这种数据结构的特点是数据元素之间是 1 对 N 的联系，即树结构。

    4. D = {01, 02, 03, 04, 05}
       S = {<01,02>, <01,05>, <02,01>, <02,03>, <02,04>, <03,02>,<04,02>, <04,05>, <05,01>, <05,04>}:
       每个数据元素可以有多个直接前驱元素，也可以有多个直接后续元素。这种数据结构的特点是数据元素之间是 M 对 N 的联系，即图结构。

       ![img](1.%E7%BB%AA%E8%AE%BA.assets/20180709200226674.png)

- 数据元素及其关系在计算机存储内的表示，即数据的存储表示 (存储结构)；

  - 数据结构在计算机存储器中的存储映像，又叫物理结构

  - 存储结构既要反应数据元素本身，还要反映数据元素之间的关系

  - 一般有两种存储结构：顺序存储结构与链式存储结构

    ![image-20210924100904309](1.%E7%BB%AA%E8%AE%BA.assets/image-20210924100904309.png)

  - 索引存储：在存储元素信息的同时建立附加的索引表

  - 散列存储：根据节点的关键码通过一个散列函数计算得到存储地址

  - ![image-20210924101203860](1.%E7%BB%AA%E8%AE%BA.assets/image-20210924101203860.png)

- 数据的运算，即对数据元素施加的操作。

  - 建立：建立某种制定的数据结构
  - 清除：把某个指定的数据结构置为空
  - 求长：求指定的数据结构中数据元素的个数
  - 判空：判定数据结构是否为空
  - 判满：判定数据结构是否达到逻辑或存储的最大允许容量
  - 获取：获取数据结构中某个制定位置的数据元素
  - 更新：修改数据结构中某个数据元素的值
  - 插入：在数据结构指定位置插入一个新的数据元素
  - 删除：在数据结构指定位置删除一个数据元素
  - 查找：在数据结构中查找某个满足条件的数据元素
  - 遍历：遍历数据结构中所有数据元素，处理方法由函数对象指定

## 渐进复杂度

### 大O记号

考察一个DSA时，更多地要考察它的潜力与“长远”，即考察它在处理更大规模问题时的表现。

渐进分析：随着问题规模的增长，计算成本如何增长？  这里主要考察：问题规模足够大之后，计算成本的增长趋势 。

即：当输入规模 n ≫ 2 后，算法需执行的基本操作次数 T(n) = ?  

Paul Bachmann, 1894: 
$$
T(n)=\mathcal{O}(f(n))\space iff \space \exists c>0 s.t.T(n)<c \cdot f(n) \quad \forall n \gg 2
$$
如：
$$
\sqrt{5 n \cdot[3 n \cdot(n+2)+4]+6}<\sqrt{5 n \cdot\left[\underline{6 n^{2}}+4\right]+6}<\sqrt{\underline{35 n^{3}+6}}<6 \cdot n^{1.5}=O\left(n^{1.5}\right)
$$
与T(n)相比， f(n)在形式上更为简洁，但依然反映前者的增长趋势  

- 忽略常系数
  $$
  \mathcal{O}(f(n))=\mathcal{O}(c \cdot f(n))
  $$

- 忽略低次项：
  $$
  \mathcal{O}\left(n^{a}+n^{b}\right)=\mathcal{O}\left(n^{a}\right), a \geq b>0
  $$

还有其他记号：
$$
T(n)=\Omega(f(n)) \quad \text { iff } \quad \exists c>0 \quad \text { s.t. } \quad T(n)>c \cdot f(n) \quad \forall n \gg 2
$$

$$
T(n)=\Theta(f(n)) \text { iff } \quad \exists c_{1}>c_{2}>0 \quad \text { s.t. } \quad c_{1} \cdot f(n)>T(n)>c_{2} \cdot f(n) \quad \forall n \gg 2
$$

![image-20210918160437245](1.%E7%BB%AA%E8%AE%BA.assets/image-20210918160437245.png)

### 可解的问题

#### O(1)：常数复杂度

常数： 2 = 2021 = 2021 * 2021 = O(1)  

甚至$2021^{2021}=O(1)$

> 从渐近的角度来看， 再大的常数，也要小于递增的变数  ，尽管实际并非如此。

常数复杂度：随着问题规模n的增长，计算的时间成本不变。

这类算法的效率最高

#### $O(log^c n)$对数多项式复杂度

对数：$\mathcal{O}(\log n)$，包括$\ln n \quad \lg n \quad \log _{100} n \quad \log _{2021} n$

为何不注明底数？

- 常底数无所谓
  $$
  \forall a, b>1, \log _{a} n=\log _{a} b \cdot \log _{b} n=\Theta\left(\log _{b} n\right)
  $$

- 常数次幂无所谓
  $$
  \forall c>0, \log n^{c}=c \cdot \log n=\Theta(\log n)
  $$

- 对数多项式：
  $$
  123 \cdot \log ^{321} n+\log ^{205}\left(7 \cdot n^{2}-15 \cdot n+31\right)=\Theta\left(\log ^{321} n\right)
  $$

这类算法非常有效，复杂度无限接近于常数。
$$
\forall c>0, \quad \log n=\mathcal{O}\left(n^{c}\right)
$$

#### $O\left(n^{c}\right)$多项式复杂度

多项式：$a_{k} \cdot n^{k}+a_{k-1} \cdot n^{k-1}+\cdots+a_{2} \cdot n^{2}+a_{1} \cdot n+a_{0}=\mathcal{O}\left(n^{k}\right), \quad a_{k}>0$
$$
\sqrt{23 \cdot n-472} \times \sqrt{101 \cdot n+203}=\mathcal{O}(n)
$$

$$
\sqrt[3]{2 \cdot n^{3}-\sqrt[3]{3 \cdot n^{4}-\sqrt{4 \cdot n^{5}+\sqrt{5 \cdot n^{6}+\sqrt{6 \cdot n^{7}+\sqrt{7 \cdot n^{8}+\sqrt{8 \cdot n^{9}+n^{2019} / \sqrt{n^{6}-5 \cdot n^{3}+1970}}}}}}}}=\mathcal{O}\left(n^{7}\right)
$$

线性复杂度（linear function）：所有O（n）类函数

这类算法的效率通常认为已可令人满意，然而这个标准是否太低了？  不是，因为这是一个可解的问题，至少不是难解的问题。

### 难解的问题

#### $O(2^n)$指数复杂度

指数复杂度是和多项式复杂度是有天壤之别的，多项式复杂度往往能够被指数复杂度所覆盖。

指数：
$$
T(n)=\mathcal{O}\left(a^{n}\right), \quad a>1
$$

$$
\begin{aligned}
&\because e^{n}=1+n+n^{2} / 2 !+n^{3} / 3 !+n^{4} / 4 !+\ldots \\
&\therefore \forall c>1, n^{c}=\mathcal{O}\left(2^{n}\right)
\end{aligned}
$$

这类算法的计算成本增长极快，通常被认为不可忍受  。

从$O(n^{c}) $到$O\left(2^{n}\right)$是从**有效算法**到**无效算法**的分水岭。 

$O\left(2^{n}\right)$算法往往显而易见，然而设计出$O(n^{c}) $算法却极其不易，有时甚至注定是徒劳无功  

举例：2-Subset问题
$$
\begin{aligned}
&\forall S=\left\{a_{1}, a_{2}, \ldots, a_{n}\right\} \subset \mathbb{Z} \text { and } \sum_{k=1}^{n} a_{k}=2 m \\
&\exists T \subset S \text { s.t. } \sum_{a \in T} a=m=\sum_{a \notin T} a ?
\end{aligned}
$$
直觉上，并不难：逐一枚举S的每一子集，统计总和并核对  

而这种“算法”的计算成本是：
$$
\left|2^{S}\right|=2^{|S|}=2^{n}
$$
这是一个“难解”的算法，故严格地讲，这仍只是程序，而不是算法  .

还有更好的算法吗？

2-Subset is NP-complete .就目前的计算模型而言， 不存在可在多项式时间内回答此问题的算法  ——就此意义而言，上述的直觉算法已属最优  。

P就是能在多项式时间内解决的问题，NP就是能在多项式时间验证答案正确与否的问题。所以P是否等于NP实质上就是在问，如果对于一个问题我能在多项式时间内验证其答案的正确性，那么我是否能在多项式时间内解决它？这个表述不太严谨，但通俗来讲就是如此。

#### 算法复杂度层次

O(1) < O(logn) < O(√n) < O(n) < O(nlogn) < O(n2) < O(n3) < O(2n) < O(n!)

### P问题、NP问题和NPC问题

 还是先用几句话简单说明一下时间复杂度。时间复杂度并不是表示一个程序解决问题需要花多少时间，而是当问题规模扩大后，程序需要的时间长度增长得有多快。也就是说，对于高速处理数据的计算机来说，处理某一个特定数据的效率不能衡量一个程序的好坏，而应该看当这个数据的规模变大到数百倍后，程序运行时间是否还是一样，或者也跟着慢了数百倍，或者变慢了数万倍。不管数据有多大，程序处理花的时间始终是那么多的，我们就说这个程序很好，具有O(1)的时间复杂度，也称常数级复杂度；数据规模变得有多大，花的时间也跟着变得有多长，这个程序的时间复杂度就是O(n)，比如找n个数中的最大值；而像冒泡排序、插入排序等，数据扩大2倍，时间变慢4倍的，属于$O(n^2)$的复杂度。还有一些穷举类的算法，所需时间长度成几何阶数上涨，这就是$O(a^n)$的指数级复杂度，甚至O(n!)的阶乘级复杂度。不会存在$O(2*n^2)$的复杂度，因为前面的那个“2”是系数，根本不会影响到整个程序的时间增长。同样地，$O (n^3+n^2)$的复杂度也就是$O(n^3)$的复杂度。因此，我们会说，一个$O(0.01*n^3)$的程序的效率比$O(100*n^2)$的效率低，尽管在n很小的时候，前者优于后者，但后者时间随数据规模增长得慢，最终$O(n^3)$的复杂度将远远超过$O(n^2)$。我们也说，$O(n^{100})$的复杂度小于O(1.01^n)的复杂度。

容易看出，前面的几类复杂度被分为两种级别，其中后者的复杂度无论如何都远远大于前者：一种是O(1),O(log(n)),$O(n^a)$等，我们把它叫做多项式级的复杂度，因为它的规模n出现在底数的位置；另一种是$O(a^n)$和O(n!)型复杂度，它是非多项式级的，其复杂度计算机往往不能承受。当我们在解决一个问题时，我们选择的算法通常都需要是多项式级的复杂度，非多项式级的复杂度需要的时间太多，往往会超时，除非是数据规模非常小。

自然地，人们会想到一个问题：会不会所有的问题都可以找到复杂度为多项式级的算法呢？很遗憾，答案是否定的。有些问题甚至根本不可能找到一个正确的算法来，这称之为“不可解问题”(Undecidable Decision Problem)。The Halting Problem就是一个著名的不可解问题，在我的Blog上有过专门的介绍和证明。再比如，输出从1到n这n个数的全排列。不管你用什么方法，你的复杂度都是阶乘级，因为你总得用阶乘级的时间打印出结果来。有人说，这样的“问题”不是一个“正规”的问题，正规的问题是让程序解决一个问题，输出一个“YES”或“NO”（这被称为判定性问题），或者一个什么什么的最优值（这被称为最优化问题）。那么，根据这个定义，我也能举出一个不大可能会有多项式级算法的问题来：Hamilton回路。问题是这样的：给你一个图，问你能否找到一条经过每个顶点一次且恰好一次（不遗漏也不重复）最后又走回来的路（满足这个条件的路径叫做Hamilton回路）。这个问题现在还没有找到多项式级的算法。事实上，这个问题就是我们后面要说的NPC问题。

 下面引入P类问题的概念：如果一个问题可以找到一个能在多项式的时间里解决它的算法，那么这个问题就属于P问题。P是英文单词多项式的第一个字母。哪些问题是P类问题呢？通常NOI和NOIP不会出不属于P类问题的题目。我们常见到的一些信息奥赛的题目都是P问题。道理很简单，一个用穷举换来的非多项式级时间的超时程序不会涵盖任何有价值的算法。

接下来引入NP问题的概念。这个就有点难理解了，或者说容易理解错误。在这里强调（回到我竭力想澄清的误区上），NP问题不是非P类问题。NP问题是指可以在多项式的时间里验证一个解的问题。NP问题的另一个定义是，可以在多项式的时间里猜出一个解的问题。比方说，我RP很好，在程序中需要枚举时，我可以一猜一个准。现在某人拿到了一个求最短路径的问题，问从起点到终点是否有一条小于100个单位长度的路线。它根据数据画好了图，但怎么也算不出来，于是来问我：你看怎么选条路走得最少？我说，我RP很好，肯定能随便给你指条很短的路出来。然后我就胡乱画了几条线，说就这条吧。那人按我指的这条把权值加起来一看，嘿，神了，路径长度98，比100小。于是答案出来了，存在比100小的路径。别人会问他这题怎么做出来的，他就可以说，因为我找到了一个比100 小的解。在这个题中，找一个解很困难，但验证一个解很容易。验证一个解只需要O(n)的时间复杂度，也就是说我可以花O(n)的时间把我猜的路径的长度加出来。那么，只要我RP好，猜得准，我一定能在多项式的时间里解决这个问题。我猜到的方案总是最优的，不满足题意的方案也不会来骗我去选它。这就是NP问题。当然有不是NP问题的问题，即你猜到了解但是没用，因为你不能在多项式的时间里去验证它。下面我要举的例子是一个经典的例子，它指出了一个目前还没有办法在多项式的时间里验证一个解的问题。很显然，前面所说的Hamilton回路是NP问题，因为验证一条路是否恰好经过了每一个顶点非常容易。但我要把问题换成这样：试问一个图中是否不存在Hamilton回路。这样问题就没法在多项式的时间里进行验证了，因为除非你试过所有的路，否则你不敢断定它“没有Hamilton回路”。

之所以要定义NP问题，是因为通常只有NP问题才可能找到多项式的算法。我们不会指望一个连多项式地验证一个解都不行的问题存在一个解决它的多项式级的算法。相信读者很快明白，信息学中的号称最困难的问题——“NP问题”，实际上是在探讨NP问题与P类问题的关系。

很显然，所有的P类问题都是NP问题。也就是说，能多项式地解决一个问题，必然能多项式地验证一个问题的解——既然正解都出来了，验证任意给定的解也只需要比较一下就可以了。关键是，人们想知道，是否所有的NP问题都是P类问题。我们可以再用集合的观点来说明。如果把所有P类问题归为一个集合P中，把所有 NP问题划进另一个集合NP中，那么，显然有P属于NP。现在，所有对NP问题的研究都集中在一个问题上，即究竟是否有P=NP？通常所谓的“NP问题”，其实就一句话：证明或推翻P=NP。

 NP问题一直都是信息学的巅峰。巅峰，意即很引人注目但难以解决。在信息学研究中，这是一个耗费了很多时间和精力也没有解决的终极问题，好比物理学中的大统一和数学中的歌德巴赫猜想等。

目前为止这个问题还“啃不动”。但是，一个总的趋势、一个大方向是有的。人们普遍认为，P=NP不成立，也就是说，多数人相信，存在至少一个不可能有多项式级复杂度的算法的NP问题。人们如此坚信P≠NP是有原因的，就是在研究NP问题的过程中找出了一类非常特殊的NP问题叫做NP-完全问题，也即所谓的 NPC问题。C是英文单词“完全”的第一个字母。正是NPC问题的存在，使人们相信P≠NP。下文将花大量篇幅介绍NPC问题，你从中可以体会到NPC问题使P=NP变得多么不可思议。

为了说明NPC问题，我们先引入一个概念——约化(Reducibility，有的资料上叫“归约”)。

简单地说，一个问题A可以约化为问题B的含义即是，可以用问题B的解法解决问题A，或者说，问题A可以“变成”问题B。《算法导论》上举了这么一个例子。比如说，现在有两个问题：求解一个一元一次方程和求解一个一元二次方程。那么我们说，前者可以约化为后者，意即知道如何解一个一元二次方程那么一定能解出一元一次方程。我们可以写出两个程序分别对应两个问题，那么我们能找到一个“规则”，按照这个规则把解一元一次方程程序的输入数据变一下，用在解一元二次方程的程序上，两个程序总能得到一样的结果。这个规则即是：两个方程的对应项系数不变，一元二次方程的二次项系数为0。按照这个规则把前一个问题转换成后一个问题，两个问题就等价了。同样地，我们可以说，Hamilton回路可以约化为TSP问题(Travelling Salesman Problem，旅行商问题)：在Hamilton回路问题中，两点相连即这两点距离为0，两点不直接相连则令其距离为1，于是问题转化为在TSP问题中，是否存在一条长为0的路径。Hamilton回路存在当且仅当TSP问题中存在长为0的回路。

 “问题A可约化为问题B”有一个重要的直观意义：B的时间复杂度高于或者等于A的时间复杂度。也就是说，问题A不比问题B难。这很容易理解。既然问题A能用问题B来解决，倘若B的时间复杂度比A的时间复杂度还低了，那A的算法就可以改进为B的算法，两者的时间复杂度还是相同。正如解一元二次方程比解一元一次方程难，因为解决前者的方法可以用来解决后者。

很显然，约化具有一项重要的性质：约化具有传递性。如果问题A可约化为问题B，问题B可约化为问题C，则问题A一定可约化为问题C。这个道理非常简单，就不必阐述了。 现在再来说一下约化的标准概念就不难理解了：如果能找到这样一个变化法则，对任意一个程序A的输入，都能按这个法则变换成程序B的输入，使两程序的输出相同，那么我们说，问题A可约化为问题B。

当然，我们所说的“可约化”是指的可“多项式地”约化(Polynomial-time Reducible)，即变换输入的方法是能在多项式的时间里完成的。约化的过程只有用多项式的时间完成才有意义。

好了，从约化的定义中我们看到，一个问题约化为另一个问题，时间复杂度增加了，问题的应用范围也增大了。通过对某些问题的不断约化，我们能够不断寻找复杂度更高，但应用范围更广的算法来代替复杂度虽然低，但只能用于很小的一类问题的算法。再回想前面讲的P和NP问题，联想起约化的传递性，自然地，我们会想问，如果不断地约化上去，不断找到能“通吃”若干小NP问题的一个稍复杂的大NP问题，那么最后是否有可能找到一个时间复杂度最高，并且能“通吃”所有的 NP问题的这样一个超级NP问题？答案居然是肯定的。也就是说，存在这样一个NP问题，所有的NP问题都可以约化成它。换句话说，只要解决了这个问题，那么所有的NP问题都解决了。这种问题的存在难以置信，并且更加不可思议的是，这种问题不只一个，它有很多个，它是一类问题。这一类问题就是传说中的NPC 问题，也就是NP-完全问题。NPC问题的出现使整个NP问题的研究得到了飞跃式的发展。我们有理由相信，NPC问题是最复杂的问题。再次回到全文开头，我们可以看到，人们想表达一个问题不存在多项式的高效算法时应该说它“属于NPC问题”。

NPC问题的定义非常简单。同时满足下面两个条件的问题就是NPC问题。首先，它得是一个NP问题；然后，所有的NP问题都可以约化到它。证明一个问题是 NPC问题也很简单。先证明它至少是一个NP问题，再证明其中一个已知的NPC问题能约化到它（由约化的传递性，则NPC问题定义的第二条也得以满足；至于第一个NPC问题是怎么来的，下文将介绍），这样就可以说它是NPC问题了。

既然所有的NP问题都能约化成NPC问题，那么只要任意一个NPC问题找到了一个多项式的算法，那么所有的NP问题都能用这个算法解决了，NP也就等于P 了。因此，给NPC找一个多项式算法太不可思议了。因此，前文才说，“正是NPC问题的存在，使人们相信P≠NP”。我们可以就此直观地理解，NPC问题目前没有多项式的有效算法，只能用指数级甚至阶乘级复杂度的搜索。

顺便讲一下NP-Hard问题。NP-Hard问题是这样一种问题，它满足NPC问题定义的第二条但不一定要满足第一条（就是说，NP-Hard问题要比 NPC问题的范围广）。NP-Hard问题同样难以找到多项式的算法，但它不列入我们的研究范围，因为它不一定是NP问题。即使NPC问题发现了多项式级的算法，NP-Hard问题有可能仍然无法得到多项式级的算法。事实上，由于NP-Hard放宽了限定条件，它将有可能比所有的NPC问题的时间复杂度更高从而更难以解决。

不要以为NPC问题是一纸空谈。NPC问题是存在的。确实有这么一个非常具体的问题属于NPC问题。下文即将介绍它。

下文即将介绍逻辑电路问题。这是第一个NPC问题。其它的NPC问题都是由这个问题约化而来的。因此，逻辑电路问题是NPC类问题的“鼻祖”。逻辑电路问题是指的这样一个问题：给定一个逻辑电路，问是否存在一种输入使输出为True。

逻辑电路问题属于NPC问题。这是有严格证明的。它显然属于NP问题，并且可以直接证明所有的NP问题都可以约化到它（不要以为NP问题有无穷多个将给证明造成不可逾越的困难）。证明过程相当复杂，其大概意思是说任意一个NP问题的输入和输出都可以转换成逻辑电路的输入和输出（想想计算机内部也不过是一些 0和1的运算），因此对于一个NP问题来说，问题转化为了求出满足结果为True的一个输入（即一个可行解）。

有了第一个NPC问题后，一大堆NPC问题就出现了，因为再证明一个新的NPC问题只需要将一个已知的NPC问题约化到它就行了。后来，Hamilton 回路成了NPC问题，TSP问题也成了NPC问题。现在被证明是NPC问题的有很多，任何一个找到了多项式算法的话所有的NP问题都可以完美解决了。因此说，正是因为NPC问题的存在，P=NP变得难以置信。P=NP问题还有许多有趣的东西，有待大家自己进一步的挖掘。攀登这个信息学的巅峰是我们这一代的终极目标。现在我们需要做的，至少是不要把概念弄混淆了。

### 增长速度

![image-20210921153554100](1.%E7%BB%AA%E8%AE%BA.assets/image-20210921153554100.png)

![image-20210921153601740](1.%E7%BB%AA%E8%AE%BA.assets/image-20210921153601740.png)

![image-20210921153635027](1.%E7%BB%AA%E8%AE%BA.assets/image-20210921153635027.png)

## 复杂度分析

算法分析两个主要任务 = 正确性（不变性 $\times$单调性） + 复杂度

为确定后者，真地需要将算法描述为RAM的基本指令， 再累计各条代码的执行次数？其实是没有必要的。因为在渐进复杂度的前提下，C++等高级语言的基本指令，均等效于常数条RAM的基本指令； 在渐近意义下，二者大体相当。

主要方法： 迭代（级数求和）、递归（递归跟踪 + 递推方程）、实用（猜测 + 验证）  

### 级数

- 算术级数：与末项平方同阶 
  $$
  T(n)=1+2+\ldots+n = \frac{n(n+1)}{2}=\mathcal{O}\left(n^{2}\right)
  $$

- 幂方级数：比幂次高出一阶
  $$
  \sum_{k=0}^{n} k^{d} \approx \int_{0}^{n} x^{d} d x=\left.\frac{x^{d+1}}{d+1}\right|_{0} ^{n}=\frac{n^{d+1}}{d+1}=\mathcal{O}\left(n^{d+1}\right)
  $$

  $$
  \sum_{k=1}^{n} k^{2}=1^{2}+2^{2}+3^{2}+\ldots+n^{2}=n(n+1)(2 n+1) / 6=\mathcal{O}\left(n^{3}\right)
  $$

  $$
  \sum_{k=1}^{n} k^{3}=1^{3}+2^{3}+3^{3}+\ldots+n^{3}=n^{2}(n+1)^{2} / 4=\mathcal{O}\left(n^{4}\right)
  $$

- 几何级数：与末项同阶
  $$
  T_{a}(n)=\sum_{k=0}^{n} a^{k}=a^{0}+a^{1}+a^{2}+a^{3}+\ldots+a^{n}=\frac{a^{n+1}-1}{a-1}=\mathcal{O}\left(a^{n}\right), \quad 1<a
  $$

  $$
  T_{2}(n)=\sum_{k=0}^{n} 2^{k}=1+2+4+8+\ldots+2^{n}=2^{n+1}-1=\mathcal{O}\left(2^{n+1}\right)=\mathcal{O}\left(2^{n}\right)
  $$

以及收敛级数，为O(1)。

还有不收敛，但有限的级数：

- 调和级数：
  $$
  h(n)=\sum_{k=1}^{n} \frac{1}{k}=1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\ldots+\frac{1}{n}=\ln n+\gamma+\mathcal{O}\left(\frac{1}{2 n}\right)=\Theta(\log n)
  $$

- 对数级数
  $$
  \sum_{k=1}^{n} \ln k=\ln \prod_{k=1}^{n} k=\ln n ! \approx(n+0.5) \cdot \ln n-n=\Theta(n \cdot \log n)
  $$

- 对数+线性+指数
  $$
  \sum_{k=1}^{n} k \cdot \log k \approx \int_{1}^{n} x \ln x d x=\left.\frac{x^{2} \cdot(2 \cdot \ln x-1)}{4}\right|_{1} ^{n}=\mathcal{O}\left(n^{2} \log n\right)
  $$

  $$
  \begin{aligned}
  \sum_{k=1}^{n} k \cdot 2^{k} &=\sum_{k=1}^{n} k \cdot 2^{k+1}-\sum_{k=1}^{n} k \cdot 2^{k}=\sum_{k=1}^{n+1}(k-1) \cdot 2^{k}-\sum_{k=1}^{n} k \cdot 2^{k} \\
  &=n \cdot 2^{n+1}-\sum_{k=1}^{n} 2^{k}=n \cdot 2^{n+1}-\left(2^{n+1}-2\right)=(n-1) \cdot 2^{n+1}+2=\mathcal{O}\left(n \cdot 2^{n}\right)
  \end{aligned}
  $$

### 循环

```C++
for(int i = 0; i < n; i++){
    for(int j = 0; j < n; j++){
        operation(i,j);
    }
}
```

$$
\sum_{i=0}^{n-1} n=n \times n=\mathcal{O}\left(n^{2}\right)
$$

![image-20210924005247530](1.%E7%BB%AA%E8%AE%BA.assets/image-20210924005247530.png)

```C++
for(int i = 0; i < n; i++){
    for(int j = 0; j < i; j++){
        operation(i,j);
    }
}
```

$$
\sum_{i=0}^{n-1} i=\frac{n(n-1)}{2}=\mathcal{O}\left(n^{2}\right)
$$

![image-20210924005332950](1.%E7%BB%AA%E8%AE%BA.assets/image-20210924005332950.png)

```C++
for(int i = 0; i < n; i++){
    for(int j = 0; j < i; j += 2021){
        operation(i,j);
    }

```

![image-20210924005925339](1.%E7%BB%AA%E8%AE%BA.assets/image-20210924005925339.png)

```C++
for(int i = 1; i < n; i << = 1)//这个是左移一位的操作，相当于乘2
{
    for(int j = 0; j < i; j++){
        operation(i,j);
}
```

$$
\begin{aligned}
&1+2+4+\cdots+2^{\left\lfloor\log _{2}(n-1)\right\rfloor} \\
&=2^{\left\lceil\log _{2} n\right\rceil}-1=\mathcal{O}(n)
\end{aligned}
$$

判断以下程序的复杂度：

```C++
int fun(int n){
    int i = 1;
    int s = 1;
    while(s < n*n){
        s += ++i;
    }
    return s;
}
```

程序的复杂度应为$O(n)$，而不是$O(n^2)$，不能仅关注到递归终止条件，还要注意到中间的运算过程是`s += ++i;`，两相综合，复杂度应为$O(n)$。

![image-20220101225929575](1.%E7%BB%AA%E8%AE%BA.assets/image-20220101225929575.png)

这两端程序在功能和运行结果上是相同的，但是复杂度却不同。上部分程序的复杂度应为$O(2^n)$，而下部分程序的复杂的应为$O(n)$。

### 封底估算（back-of-the-envelope calculation)

## 迭代与递归

看似高明的递归算法效率反而不是很高，而看似笨拙的迭代算法却可能体现出更强的优势。

### 减而治之（Decrease and conquer）

![image-20210924105400592](1.%E7%BB%AA%E8%AE%BA.assets/image-20210924105400592.png)

为求解一个大规模的问题，可以：

- 将其划分为两个子问题：其一平凡，另一规模缩减
- 分别求解子问题；再由子问题的解，得到原问题的解

### 递归跟踪

绘出计算过程中出现过的所有递归实例（及其调用关系）。这些递归过程各自所需的时间之总和，即为整体运行时间。但是这只适用于一些递归形式比较简单，可以“一眼看出来”的递归算法。

### 递推方程

对于大规模的问题、复杂的递归算法，递归跟踪不再适用。此时可采用另一抽象的方法...  

从递推的角度看，为求解规模为n的问题sum(A,n)，需 //T(n)

- 递归求解规模为n-1的问题sum(A,n-1)，再//T(n-1)
- 累加上A[n-1] //O(1)

递推方程：
$$
T(n)=T(n-1)+\mathcal{O}(1)//recurrence
$$

$$
T(0)=\mathcal{O}(1)//base:sum(A,0)
$$

求解：
$$
T(n)=T(n-2)+\mathcal{O}(2)=T(n-3)+\mathcal{O}(3)=\ldots=T(0)+\mathcal{O}(n)=\mathcal{O}(n)
$$

### 分而治之（Divide and conquer）

为求解一个大规模的问题，可以将其划分为若干子问题（通常两个，且规模大体相当）。分别求解子问题后，由子问题的解，合并得到原问题的解。

![image-20210924114606906](1.%E7%BB%AA%E8%AE%BA.assets/image-20210924114606906.png)

数组的二分求和：

```C++
int sum(int A[], int lo, int hi){
    if(lo == hi){
        return A[lo];
    }
    int mi = (lo + hi) >> 1;//右移操作，相当于除2
    return sum(A,lo,mi) + sum(A,mi + 1, hi);
}//入口形式为sum(A, 0, n-1)
```

![image-20210924115133971](1.%E7%BB%AA%E8%AE%BA.assets/image-20210924115133971.png)

分析复杂度：

![image-20210924115252400](1.%E7%BB%AA%E8%AE%BA.assets/image-20210924115252400.png)

![image-20210924115309087](1.%E7%BB%AA%E8%AE%BA.assets/image-20210924115309087.png)

### Master Theorem

分治策略对应的递推式， 通常（尽管不总是）形如：
$$
T(n)=a \cdot T(n / b)+\mathcal{O}(f(n))
$$
（原问题被分为a个规模均为n/b的子任务；任务的划分、解的合并耗时f(n)）  

在前面我们用recursion tree来计算递归的时间复杂度：

![img](1.%E7%BB%AA%E8%AE%BA.assets/v2-fe68d88f93eff841ea5dad0db46950eb_720w.jpg)

![img](1.%E7%BB%AA%E8%AE%BA.assets/v2-0450d4485872baf2b5dc7eda0848d8b6_720w.jpg)

这里有疑问，为什么同样的分析方法，一会儿是$O(n^2)$一会儿是$O(logn)$呢？

我们来看递归公式，可以写成这样的通用形式：
$$
T(n)=a T\left(\frac{n}{b}\right)+O\left(n^{d}\right)
$$
可以猜测，最后的时间复杂度和这a, b ,d几个参数有关。

把递归问题每一层的问题数目，计算量都一一列出，可以得到如下公式

![img](1.%E7%BB%AA%E8%AE%BA.assets/v2-fe97dcb5c9ec0962a2823b94110f5098_720w.jpg)

最后的公式是一个累加函数
$$
\text { Total work }=\sum_{0}^{\log _{b} n} O\left(n^{d}\right)\left(\frac{a}{b^{d}}\right)^{i}
$$
一看结构，是一个等比数列，公比为$\left(\frac{a}{b^{d}}\right)$ 

分析时间复杂度，比较简单，对于等比数列来说，如果公比q<1, 那么第一项最大，其他项会依次减少，最终的时间复杂度就是
$$
O\left(r_{1}\right) (r_{1} \text { 是等比数列的首项 })
$$
所以，当公比
$$
\left(\frac{a}{b^{d}}\right)<1 \Rightarrow d>\log _{b} a
$$
的时候，整个算法的时间复杂度由第一项决定，
$$
\text { Total work }=O\left(O\left(n^{d}\right)\left(\frac{a}{b^{d}}\right)^{0}\right)=O\left(n^{d}\right)
$$



而如果反过来，公比q>1, 那么最后一项最大，最终的时间复杂度由最后一项$O\left(r_{n}\right)$统治（dominate）
$$
\left(\frac{a}{b^{d}}\right)>1 \Rightarrow d<\log _{b} a
$$
整个算法的时间复杂度由最后一项决定
$$
\text { Total work }=O\left(O\left(n^{d}\right)\left(\frac{a}{b^{d}}\right)^{\log _{b} n}\right)=O\left(n^{\log _{b} a}\right)
$$
公比q =1， 每一项都一样大，这样子就要求和了。

所以
$$
\text { Total work }=O\left(n^{d}\right)\left(1+\log _{b} n\right)=O\left(n^{d} \log _{b} n\right)
$$

## 动态规划

### 斐波那契数列

$$
f i b(n)=f i b(n-1)+f i b(n-2)
$$

```C++
int fib(n){
    return (2>n) ? n : fib(n-1) + fib(n-2);
}
```

 分析复杂度：
$$
T(0)=T(1)=1
$$

$$
T(n)=T(n-1)+T(n-2)+1, \quad \forall n>1
$$

令：
$$
S(n)=[T(n)+1] / 2
$$
则：
$$
S(0)=1=f i b(1), \quad S(1)=1=f i b(2)
$$
故：
$$
S(n)=S(n-1)+S(n-2)=f i b(n+1)
$$

$$
T(n)=2 \cdot S(n)-1=2 \cdot f i b(n+1)-1=\mathcal{O}(f i b(n+1))=\mathcal{O}\left(\phi^{n}\right)
$$

其中$\phi=(1+\sqrt{5}) / 2 \approx 1.618$

（斐波那契数列的时间成本本身就呈现出斐波那契数的样子）

通过递归跟踪我们不难发现，递归版斐波那契数列低效的根源在于，各递归实例均被大量重复地调用

先后出现的递归实例, 共计 $\mathcal{O}\left(\phi^{n}\right)$ 个 $;$ 而去除重复之后, 总共不过 $\mathcal{O}(n)$ 种

#### 记忆法：Memoization

将已计算过实例的结果制表备查

#### 动态规划：dynamic programming

颠倒计算方向：由自顶向下递归，改为自底而上迭代

![image-20210927013915997](1.%E7%BB%AA%E8%AE%BA.assets/image-20210927013915997.png)

```C++
f = 1;
g = 0;//fib(-1),fib(0)
while (0 < n--){
    g = g + f;
    f = g - f;
}
return g;
```

$T(n) = O(n)$,而且仅需O（1）空间！

## 局限

### 缓存

就地循环位移：

仅用O(1)辅助空间，将数组A[0, n)中的元素向左循环移动k个单元  

> 这个O（1）正是“就地”的要求

```C++
void shift(int* A, int n, int k);
```

![image-20210927022001742](1.%E7%BB%AA%E8%AE%BA.assets/image-20210927022001742.png)

蛮力版：

```C++
void shift0(int* A, int n, int k)//反复以1为间距循环左移
{
    while(k--){
        shift(A,n,0,1); //共迭代k次，O(n*k)
    }
}
```

![image-20210927022319779](1.%E7%BB%AA%E8%AE%BA.assets/image-20210927022319779.png)

最大公约数迭代法：

由于n，k已知，所以在移动后每个数字的destination我们是确定的。

```C++
void shift1(int* A, int n, int k){
    for(int s = 0, mov = 0; mov < n; s++){
        mov += shift (A,n,s,k);
    }
}
```

![image-20210927022740882](1.%E7%BB%AA%E8%AE%BA.assets/image-20210927022740882.png)

经多轮迭代，实现数组循环左移k位，累计O（n+g）。

其中：
$$
O(g)=O(G C D(n, k))
$$
倒置法：

```C++
void shift2(int* A, int n, int k){
    reverse(A, k);
    reverse(A + k, n - k);
    reverse(A, n);
}
```

![image-20210927024712700](1.%E7%BB%AA%E8%AE%BA.assets/image-20210927024712700.png)

这个方法经过计算，复杂度大概是O（3n）。

看起来效果没第二个算法好，但是算起来比前两个算法快很多。为什么呢？

这正是因为cache的机制。

在思考为什么需要cache之前，我们首先先来思考另一个问题：我们的程序是如何运行起来的？

我们应该知道程序是运行在 RAM之中，RAM 就是我们常说的DDR（例如： DDR3、DDR4等）。我们称之为main memory（主存）。当我们需要运行一个进程的时候，首先会从磁盘设备（例如，eMMC、UFS、SSD等）中将可执行程序load到主存中，然后开始执行。在CPU内部存在一堆的通用寄存器（register）。如果CPU需要将一个变量（假设地址是A）加1，一般分为以下3个步骤：

1. CPU 从主存中读取地址A的数据到内部通用寄存器 x0（ARM64架构的通用寄存器之一）。
2. 通用寄存器 x0 加1。
3. CPU 将通用寄存器 x0 的值写入主存。

我们将这个过程可以表示如下：

![img](1.%E7%BB%AA%E8%AE%BA.assets/v2-1aa0caac22aec470dd15d0a7ca1f4c80_b.jpg)

其实现实中，CPU通用寄存器的速度和主存之间存在着太大的差异。两者之间的速度大致如下关系：

![img](1.%E7%BB%AA%E8%AE%BA.assets/v2-cce58cab829ecc2755f6797b41bea821_b.jpg)

CPU register的速度一般小于1ns，主存的速度一般是65ns左右。速度差异近百倍。因此，上面举例的3个步骤中，步骤1和步骤3实际上速度很慢。当CPU试图从主存中load/store 操作时，由于主存的速度限制，CPU不得不等待这漫长的65ns时间。如果我们可以提升主存的速度，那么系统将会获得很大的性能提升。如今的DDR存储设备，动不动就是几个GB，容量很大。如果我们采用更快材料制作更快速度的主存，并且拥有几乎差不多的容量。其成本将会大幅度上升。我们试图提升主存的速度和容量，又期望其成本很低，这就有点难为人了。因此，我们有一种折中的方法，那就是制作一块速度极快但是容量极小的存储设备。那么其成本也不会太高。这块存储设备我们称之为cache memory。在硬件上，我们将cache放置在CPU和主存之间，作为主存数据的缓存。 当CPU试图从主存中load/store数据的时候， CPU会首先从cache中查找对应地址的数据是否缓存在cache 中。如果其数据缓存在cache中，直接从cache中拿到数据并返回给CPU。当存在cache的时候，以上程序如何运行的例子的流程将会变成如下：

![img](1.%E7%BB%AA%E8%AE%BA.assets/v2-bc15d8c0612599fc3de51c4382e07aa5_b.jpg)

CPU和主存之间直接数据传输的方式转变成CPU和cache之间直接数据传输。cache负责和主存之间数据传输。

