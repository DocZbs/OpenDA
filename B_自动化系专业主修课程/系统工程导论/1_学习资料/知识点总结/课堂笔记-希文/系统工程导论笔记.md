
![](_assets/image-20230918194215496.png)


# 序言

**系统工程发展的几个阶段**

- 第一阶段: 源远流长
	- 大禹治水 (“治水”与“治人”并重); 田忌赛马 (系统的要素不变, 但策略不同, 则总体效果不同); 都江堰工程 (系统的思想, 整理考虑,综合寻优); 丁渭工程
- 第二阶段: 突然崛起 (大体上形成于上世纪 50、60 年代)
	- 北极星导弹核潜艇计划; 阿波罗登月计划 (计划评审技术 PERT+图解评审技术GERT)
- 第三阶段: 迅速扩张
	- 经典案例的特点: 问题明确, 规模巨大, 递归结构
	- 基本方法: 分解协调.  各子系统求解过程受协调变量影响, 大系统调节协调变量实现自己目标.
	- 用大系统方法处理社会经济问题时遇到新问题: 系统中包括有主观意志的人.
	- 在中国上世纪50、60年代开始, 80年代掀起高潮, 成功案例如载人航天工程.
- 第四阶段: 返璞归真


---



**计划评审技术 PERT**
1. 事件 (Events) 表示主要活动结束的那一点
2. 活动 (Activities) 表示从一个事件到另一个事件之间的过程
3. 松弛时间 (Slack Time) 不影响完工前提下可能被推迟完成的最大时间
4. 关键路线 (Critical Path) 是PERT网络中花费时间最长的事件和活动的序列

![](_assets/image-20231221195917826.png)

----
![](_assets/image-20231221211228874.png)

![](_assets/image-20231221211246584.png)

---

系统的定义: 系统是具特定功能的, 相互间具有有机联系的许多要素所构成的一个整体.

特性: 1)集合性 2) 整体性 3) 相关性 4)阶层性 5) 目的性 6)环境适应性

系统实现环境适应性有如下3种方式: 1) 适应性自稳 2) 适应性组织 3)自组织性

系统的分类: 自然系统和人工系统; 实体系统和概念系统; 动态系统与静态系统; 控制系统与行为系统

系统的功能: $F$, 体现了系统与外部环境之间物质能量信息交换能力; 系统的环境: $E$; 系统的结构: $S$; 系统的组成要素: $C$

$$
F=f(E, C, S)
$$

要素、结构、功能三者关系:
1. 要素不同, 功能不同;
2. 要素相同, 结构不同, 功能不同;
3. 要素, 结构不同, 也可能功能相同。
4. 同一结构, 也可多种功能。

系统工程的主要特点: 定量, 寻优; 

系统工程的基本手段: 建模, 优化.

---

系统工程的步骤:
1. 明确问题
2. 设置目标, 建立评价准则
3. 方案的生成与未来环境预测. 预测方法: 定性预测, 定量预测.
4. 把方案建模
5. 对方案进行评价
6. 选择一个方案
7. 规划实施

![](_assets/image-20231221213503671.png)

![](_assets/image-20231221213527847.png)

![](_assets/image-20231221213535250.png)

----

**系统工程与数学的关系**

![](_assets/image-20231221213630937.png)

![](_assets/image-20231221213637851.png)

![](_assets/image-20231221213643686.png)

![](_assets/image-20231221213718027.png)

![](_assets/image-20231221213732238.png)

![](_assets/image-20231221213740389.png)

![](_assets/image-20231221213801953.png)

![](_assets/image-20231221213810831.png)

![](_assets/image-20231221213817106.png)

![](_assets/image-20231221213957465.png)

![](_assets/image-20231221214005460.png)

![](_assets/image-20231221214012231.png)

![](_assets/image-20231221214023236.png)

![](_assets/image-20231221214433042.png)

![](_assets/image-20231221214453025.png)

![](_assets/image-20231221214648266.png)

![](_assets/image-20231221214655015.png)

![](_assets/image-20231221214702160.png)

![](_assets/image-20231221214711325.png)

![](_assets/image-20231221214717879.png)

![](_assets/image-20231221214725639.png)

![](_assets/image-20231221214732443.png)

![](_assets/image-20231221214738593.png)

![](_assets/image-20231221214744615.png)

![](_assets/image-20231221214756082.png)

![](_assets/image-20231221214916819.png)

![](_assets/image-20231221214923624.png)

![](_assets/image-20231221214945129.png)

![](_assets/image-20231221214954068.png)


# 系统建模


## 定性建模方法: 解释结构模型方法

本章重点:
- 基于邻接矩阵确定可达矩阵, 并用可达矩阵确定骨架图
- 手工方法确定骨架图

背景:
- 系统由要素构成, 要素之间存在逻辑关系, 并可以用一定的数学模型描述
- 要了解系统中各要素之间的关系, 需要建立系统的结构模型

### 结构模型

结构模型: 使用有向连接图来描述系统各要素间的关系，以表示一个作为要素集合体的系统的模型。
- 结构模型是一种几何模型。结构模型使用由节点和有向边构成的图来描述一个系统的结构。节点: 系统要素; 有向边: 要素之间的关系.
- 结构模型是一种以定性分析为主的模型。

![](_assets/image-20231222224620153.png)

![](_assets/image-20231222224632352.png)

-----

![](_assets/image-20231221223017168.png)

![](_assets/image-20231221223049556.png)


### 解释结构模型法 (ISM)

- 基本解释结构模型法概述
- ISM 解决的问题及问题定义
- 有向图的矩阵表示
- 有向图的可达矩阵
- 基于可达矩阵对变量做层次划分
- 分块确定骨架图

ISM 是美国 John Warfield 教授于1973年开发的.

#### 图的基本概念

图的基本概念

有向连接图: 指由若干节点和有向边连接而成的图形. 其中节点的集合是 $S$, 有向边的集合是 $E$.

![](_assets/image-20231222224854303.png)

树: 没有回路的连通图就是树.

关联树: 在节点上带有加权值 $W$, 而在边上有关联值 $r$ 的树称作关联树.

----

图的矩阵表示法

邻接矩阵: 用来描述图中各节点两两之间的关系. 邻接矩阵 $A$ 的元素 $a_{i j}$ 表示为, 若 $S_i$ 与 $S_j$ 有关系则 $a_{i j}=1$, 反之为 0. 注意顺序, 有向边从 $S_i$ 到 $S_j$ 则 $a_{i j}=1$.

![](_assets/image-20231222225345586.png)


- 矩阵 A 的元素全为零的行所对应的节点称为汇点，即只有有向边进入而没有离开该节点。如 S1。
- 矩阵 A 的元素全为零的列所对应的节点称为源点，即只有有向边离开而没有进入该节点。如 S4。
- 对应每一节点的行中，其元素值为 1 的数量，就是离开该节点的有向边数。
- 对应每一节点的列中，其元素值为1的数量，就是进入该节点的有向边数。

可达矩阵: 是指用矩阵形式来描述有向连接图各节点之间，经过一定长度的通路后可以到达的程度。

![](_assets/image-20231222225602996.png)

![](_assets/image-20231222225610203.png)

![](_assets/image-20231222225617373.png)

![](_assets/image-20231222225624055.png)

![](_assets/image-20231222225631736.png)

![](_assets/image-20231222225645251.png)

![](_assets/image-20231222225650652.png)


![](_assets/image-20231222225702514.png)

![](_assets/image-20231222225710419.png)

![](_assets/image-20231222225722049.png)

![](_assets/image-20231222225727273.png)


#### 骨架图与可达矩阵

确定骨架图的步骤: 1) 确定邻接矩阵 2)计算可达矩阵 3) 做层次划分 4) 确定骨架图

![](_assets/image-20231222232137068.png)

![](_assets/image-20231222232314605.png)

![](_assets/image-20231222232324520.png)

![](_assets/image-20231222232623955.png)

![](_assets/image-20231222232649852.png)

![](_assets/image-20231222232700048.png)

![](_assets/image-20231222232707302.png)

结论: $n$ 个变量的邻接矩阵 $A$，当 $k$ 大于或等于 $n$ 后， $A^k$ 的非对角线上不会有首次为 1 的元素。所以, $n$ 个变量的有向图，若两个变量间没有 $1 ， 2 ， \ldots, n-1$ 次通道，它们之间就不会有通道。所以, 研究变量间有无通道, 只需看 $A, A^2, \cdots, A^{n-1}$.

有向图的可达矩阵:

$$
R=I+A+A^2+\cdots+A^{n-1}
$$

- 只要变量间存在通道, $R$ 的相应元素为 1
- 若变量间不存在通道, $R$ 的相应元素为 0

而因为

$$
\begin{aligned}
(I+A)^2&=(I+A) \times(I+A) \\
&= I+A+A+A^2 \\
&=I+A+A^2
\end{aligned}
$$

$$
\begin{aligned}
(I+A)^3=&(I+A)^2 \times(I+A) \\
= & \left(I+A+A^2\right) \times(I+A) \\
= & I+A+A^2+A+A^2+A^3 \\
= & I+A+A^2+A^3
\end{aligned}
$$

所以

$$
R=(I+A)^{n-1}
$$

而如果有 $m<n-1$ 满足

$$
(I+A)^m=(I+A)^{m+1}
$$

则:

$$
R=(I+A)^m
$$

因为

$$
\begin{aligned}
(I+A)^m=&(I+A)^{m+1} \\
=& (I+A)^m(I+A) \\
= & (I+A)^{m+1}(I+A)=(I+A)^{m+2} \\
= & \cdots \\
= &(I+A)^{n-1}
\end{aligned}
$$


#### 层次划分

![](_assets/image-20231222233338708.png)

![](_assets/image-20231222233353859.png)

利用以下规则就可以确定骨架图
1. 同层变量或者互通或者不通 (根据可达矩阵判断)
2. 每层变量仅指向**相邻的**上层变量 (根据可达矩阵判断)
3. 每层变量不指向下层变量

求骨架图也就是在反复求顶层变量.

顶层变量特征: 1) 不达到其他变量 2)如能达到某个变量, 则该变量也能达到它

结论: 变量 $i$ 是顶层变量当且仅当其满足 

$$
E(i) \subset F(i)
$$

其中, ${E}({i})$ 表示变量 $i$ 能达到的变量的集合, ${F}({i})$ 表示能达到变量 $i$ 的变量的集合

![](_assets/image-20231223003204768.png)

![](_assets/image-20231223003213860.png)

![](_assets/image-20231223003219582.png)

![](_assets/image-20231223003227002.png)

![](_assets/image-20231223003237852.png)

![](_assets/image-20231223003257037.png)

>这个图的意思应该是, 刚才确定了 2, 3, 5, 8 是顶层变量, 所以就去掉了.

![](_assets/image-20231223003321029.png)

![](_assets/image-20231223003327500.png)

![](_assets/image-20231223003332362.png)

![](_assets/image-20231223003342439.png)

![](_assets/image-20231223003351370.png)


#### 确定骨架图

基本步骤: 1) 选择参考变量 2) 将所有变量逐个和参考变量比较 3) 考虑间接影响 4) 对所有变量分类 5) 以分析方法确定骨架图

![](_assets/image-20231223003953263.png)

![](_assets/image-20231223004020316.png)

![](_assets/image-20231223004030020.png)

![](_assets/image-20231223004046400.png)

![](_assets/image-20231223004052248.png)

![](_assets/image-20231223004057229.png)

![](_assets/image-20231223004102288.png)

![](_assets/image-20231223004106515.png)

![](_assets/image-20231223004112500.png)

![](_assets/image-20231223004117037.png)

![](_assets/image-20231223004126782.png)

![](_assets/image-20231223004133551.png)

![](_assets/image-20231223004138744.png)

![](_assets/image-20231223004145641.png)

---

![](_assets/image-20231223004156825.png)

![](_assets/image-20231223004201250.png)

![](_assets/image-20231223004248050.png)

![](_assets/image-20231223004255243.png)

![](_assets/image-20231223004300725.png)

![](_assets/image-20231223004306118.png)

![](_assets/image-20231223004311338.png)

![](_assets/image-20231223004315753.png)

![](_assets/image-20231223004320960.png)



## 定量建模方法: 黑箱建模

### 黑箱建模

什么是黑箱
- 不清楚物理结构, 或结构过于复杂
- 不了解机理规律, 或机理过于复杂

黑箱建模: 根据观测的输入输出数据, 寻找规律, 建立数学模型

黑箱建模 (曲面拟合, 回归)方法
- 选择由待定参数决定的一类函数 $f(x \mid \theta)$
- 获取样本数据 $x(t), y(t), t=1,2, \cdots$
- 拟合样本数据 $\min _\theta \sum_t(y(t)-f(x(t) \mid \theta))^2$
- 获得经验模型 $y \approx f(x \mid \hat{\theta})$

![](_assets/image-20231224141849309.png)

#### 多项式逼近

![](_assets/image-20231224141921916.png)

![](_assets/image-20231224141928487.png)

![](_assets/image-20231224141937623.png)

逼近能力:
- 对任意阶可导函数, 由泰勒定理保证
- 对连续函数, 由魏尔斯特拉斯定理保证

结论: 对任何连续函数, 存在可以和其任意靠近的多项式函数序列

![](_assets/image-20231224142026884.png)

采用最小二乘方法（Least Square）估计参数 $\hat{\theta}=\left(\Phi \Phi^T\right)^{-1} \Phi Y^T$

![](_assets/image-20231224142049888.png)

![](_assets/image-20231224142056313.png)

![](_assets/image-20231224142101840.png)

![](_assets/image-20231224142135672.png)

![](_assets/image-20231224142144778.png)

![](_assets/image-20231224142150833.png)

![](_assets/image-20231224142156726.png)

![](_assets/image-20231224142202219.png)

![](_assets/image-20231224142207411.png)

![](_assets/image-20231224142212274.png)

![](_assets/image-20231224142220889.png)

多项式逼近的不足: 模型好用性和逼近能力有严重矛盾!
- 增加逼近能力需要增加多项式阶次
- 当时, 当阶次较高时, 对样本点的高精度拟合, 几乎完全不能保证对非样本点的高精度预测


![](_assets/image-20231224142344155.png)

![](_assets/image-20231224142353045.png)

![](_assets/image-20231224142401214.png)

#### 基函数方法

限制基函数起作用的区域，用局部基函数代替全局基函数: 用一类函数做基函数, 其中每个基函数只在局部区域起作用.

![](_assets/image-20231224142514899.png)

高斯 RBF  $\rho(x \mid \beta(k))=\exp \left(\frac{-\left\|x-c_k\right\|^2}{\delta_k^2}\right)$

![](_assets/image-20231224142534351.png)

![](_assets/image-20231224142540330.png)

![](_assets/image-20231224142550798.png)

![](_assets/image-20231224142555152.png)

![](_assets/image-20231224142603245.png)

![](_assets/image-20231224142607445.png)

![](_assets/image-20231224142614970.png)

![](_assets/image-20231224142620695.png)

![](_assets/image-20231224142626337.png)

![](_assets/image-20231224142630450.png)

![](_assets/image-20231224142636151.png)

![](_assets/image-20231224142642229.png)

![](_assets/image-20231224142649721.png)

![](_assets/image-20231224142701323.png)

辐射基函数类神经网络: 用基函数显著大于 0 的部分

$$
\sum_k \alpha_k \eta\left(\frac{x-c_k}{\delta_k}\right) \approx \sum_k g\left(z_k\right) h s t e p_{I_k}(x)
$$

岭函数类神经网络: 用基函数接近 1 的部分, 所需要的节点数目 $M=m^n$

$$
\begin{aligned}
& \sum_k \alpha_k \sigma\left(w_k x+d_k\right) \\
\approx & g\left(z_1\right)+\sum_k\left(g\left(z_k\right)-g\left(z_{k-1}\right)\right) \sigma\left(w_k x+d_k\right)
\end{aligned}
$$



### 回归分析方法

![](_assets/image-20231224142834598.png)

#### 线性回归问题

![](_assets/image-20231224142911878.png)

![](_assets/image-20231224142917690.png)

![](_assets/image-20231224142926563.png)

对优化问题 $\min _{\theta \in R^n} \sum_{t=1}^N\left(y(t)-\theta^T x(t)\right)^2$, 如果存在 $\left(X X^T\right)^{-1}$, 最优解为 $\hat{\theta}=\left(X X^T\right)^{-1} X Y^T$

#### 一元线性回归

![](_assets/image-20231224143035213.png)

![](_assets/image-20231224143044058.png)


其中 $\hat{a}=\bar{y}-\hat{b} \bar{x}$, $\hat{b}=\frac{\sum X_i Y_i}{\sum X_i^2}=\frac{L_{x y}}{L_{x x}}$.

![](_assets/image-20231224143144214.png)

正则方程: $\frac{\partial\left(\sum e_i^2\right)}{\partial a}=-2 \sum\left(y_i-a-b x_i\right)=0$, $\frac{\partial\left(\sum e_i^2\right)}{\partial b}=-2 \sum x_i\left(y_i-a-b x_i\right)=0$.

---

**回归方程检验**

相关系数分解法

因为 $L_{y y}=\sum_{i=1}^N\left(y_i-\bar{y}\right)^2=\sum_{i=1}^N y_i^2-N \bar{y}^2=\sum_{i=1}^N\left(\hat{y}_i-\bar{y}\right)^2+\sum_{i=1}^N\left(y_i-\hat{y}_i\right)^2$, 记作总平方和 TSS = 解释平方和 ESS + 剩余平方和 RSS

定义 $r^2=\frac{E S S}{T S S}=\frac{\sum_{i=1}^N\left(\hat{y}_i-\bar{y}\right)^2}{\sum_{i=1}^N\left(y_i-\bar{y}\right)^2}$, 表示总平方和中由回归解释了的部分, 最小二乘法将使这个部分达到最大.

$r= \pm \sqrt{r^2}$ 称为相关系数, $r$ 符号与 $b$ 相同.

![](_assets/image-20231224143959241.png)

![](_assets/image-20231224144007679.png)

也可以运用假设检验方法刻画回归方程的线性因果关系, 构造统计量 $t=\frac{r \sqrt{N-2}}{\sqrt{1-r^2}}$.

设 $r$ 是总体 $(x, y)$ 的相关系数，当假设 $H_0: r=0$ 成立时，统计量 $\mathrm{t}$ 服从自由度 (degree of freedom)为 $\mathrm{N}-2$ 的 $\mathrm{t}$ 分布. 当 $t>t_\alpha$ 时，否定原假设 (null hypothesis), 认为 $x$ 与 $y$ 存在线性关系.

![](_assets/image-20231224144144275.png)

![](_assets/image-20231224144151747.png)

 F 检验法: 在假设 $H_0: b=0$ 成立时，TSS，ESS，RSS 分别是自由度为 $f_T=N-1, f_E=1, f_r=N-2$ 的 $\chi^2$ 变量，并且 RSS 与 ESS 相互独立，于是统计量 $F=\frac{E S S / f_E}{R S S / f_R}=\frac{(N-2) E S S}{R S S}$ 服从自由度为 $(1, N-2)$ 的 F 分布. 当 $F>F_\alpha$ 时，否定原假设，认为 $\mathrm{x}$ 与 $\mathrm{y}$ 存在线性关系.

![](_assets/image-20231224144258827.png)

![](_assets/image-20231224144323710.png)

---

**精度分析**

设 $S_\delta$ 为 $\mathrm{y}$ 的剩余均方差，它表示变量 $\mathrm{y}$ 偏离回归直线的误差 $S_\sigma=\sqrt{\frac{\sum_{i=1}^N\left(y_i-\hat{y}\right)^2}{N-2}}=\sqrt{\frac{\left(1-r^2\right) L_{y y}}{N-2}}$. 给定显著性水平 $\alpha$ ，对某一 $\mathrm{x}_0$ ，相应的 $\mathrm{y}_0$ 将以 $(1-\alpha)$ 的概率落在下述区间（称为置信区间）. 式中, $\hat{y}_0$ 是对应于 $x_0$ 的 $y_0$ 的预测值， $Z_{\alpha / 2}$ 是标准正态分布上 $\alpha / 2$ 百分位点的值.

![](_assets/image-20231224144544858.png)

---

![](_assets/image-20231224144604405.png)

#### 一元非线性回归

![](_assets/image-20231224144630256.png)

![](_assets/image-20231224144636498.png)

![](_assets/image-20231224144642862.png)

![](_assets/image-20231224144647873.png)

![](_assets/image-20231224144657636.png)

![](_assets/image-20231224144702426.png)

#### 多元线性回归

![](_assets/image-20231224145008360.png)

![](_assets/image-20231224145011788.png)

定义以下向量和矩阵: $\mathbf{Y}=\left[\begin{array}{llll}y_1 & y_2 & \cdots & y_N\end{array}\right]$, $\mathbf{X}=\left[\begin{array}{cccc}1 & 1 & \cdots & 1 \\ x_{11} & x_{21} & \cdots & x_{N 1} \\ x_{12} & x_{22} & \cdots & x_{N 2} \\ \vdots & \vdots & \ddots & \vdots \\ x_{1 n} & x_{2 n} & \cdots & x_{N n}\end{array}\right]$, $\boldsymbol{\beta}=\left[\begin{array}{c}\beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_n\end{array}\right]$, $\boldsymbol{\varepsilon}=\left[\begin{array}{llll}\boldsymbol{\varepsilon}_1 & \boldsymbol{\varepsilon}_2 & \cdots & \boldsymbol{\varepsilon}_N\end{array}\right]$

则回归方程及回归预测误差可表示为 $\mathbf{Y}=\boldsymbol{\beta}^{\mathrm{T}} \boldsymbol{X}+\boldsymbol{\varepsilon}$, 其中 $\hat{\boldsymbol{\beta}}=\left(\mathbf{X} \mathbf{X}^T\right)^{-1}\left(\mathbf{X} \mathbf{Y}^T\right)$

![](_assets/image-20231224145149254.png)

![](_assets/image-20231224145159636.png)

![](_assets/image-20231224145205945.png)

![](_assets/image-20231224145232479.png)

![](_assets/image-20231224145240985.png)

![](_assets/image-20231224145257725.png)

![](_assets/image-20231224145304477.png)

![](_assets/image-20231224145403365.png)

>注意这个参数是之前减去均值的建模.

---

显著性检验: 同样有: TSS=ESS+RSS

![](_assets/image-20231224150542929.png)

在假设 $\mathrm{H}_0: \beta_1=\beta_2=\cdots=\beta_n=0$ 成立时, 统计量 $F=\frac{E S S / f_E}{R S S / f_R}=\frac{(N-n-1) \cdot E S S}{n \cdot R S S}$ 服从自由度为 $(n, N-n-1)$ 的 F 分布.

当 $F>F_\alpha$ 时，否定原假设，认为 $\mathrm{x}$ 与 $\mathrm{y}$ 存在线性关系

![](_assets/image-20231224150659844.png)

![](_assets/image-20231224150707219.png)

预测精度

可以用 $S_\delta=\sqrt{\frac{R S S}{N-n-1}}$, 预测值 $\hat{y}$ 将以 $(1-\alpha)$ 的概率落在下述区域内, 即 $\left(\hat{y}_0-Z_{\alpha / 2} S_\delta, \hat{y}_0+Z_{\alpha / 2} S_\delta\right)$

![](_assets/image-20231224151500976.png)



### 病态线性回归问题

产生原因: 样本数据中回归变量间严格线性相关!

![](_assets/image-20231224151723952.png)

![](_assets/image-20231224151731890.png)

![](_assets/image-20231224151737585.png)

![](_assets/image-20231224151744187.png)

![](_assets/image-20231224151754911.png)

![](_assets/image-20231224151802324.png)

![](_assets/image-20231224151807110.png)

![](_assets/image-20231224151812285.png)

![](_assets/image-20231224151841047.png)

#### 实际病态线性回归

![](_assets/image-20231224151858101.png)

![](_assets/image-20231224151901748.png)

![](_assets/image-20231224151908259.png)

![](_assets/image-20231224151911785.png)

![](_assets/image-20231224151917224.png)

![](_assets/image-20231224151922826.png)

![](_assets/image-20231224151927027.png)

![](_assets/image-20231224151930086.png)

![](_assets/image-20231224151935023.png)

![](_assets/image-20231224151939584.png)

![](_assets/image-20231224152017243.png)

![](_assets/image-20231224152023183.png)

![](_assets/image-20231224152031437.png)

![](_assets/image-20231224152037965.png)

![](_assets/image-20231224152043222.png)

![](_assets/image-20231224152047687.png)

![](_assets/image-20231224152053885.png)

![](_assets/image-20231224152059565.png)

![](_assets/image-20231224152104045.png)

![](_assets/image-20231224152109869.png)

![](_assets/image-20231224152117341.png)

![](_assets/image-20231224152124427.png)

#### 例题

![](_assets/image-20231224152152200.png)

![](_assets/image-20231224152211570.png)

![](_assets/image-20231224152226644.png)






### 交通流量预测

![](_assets/image-20231021145051861.png)

这里应该涉及到时间序列分析的内容.

#### 移动平均法

设观测序列为 y_1, \cdots, y_T, 取移动平均的项数 N<T.

简单移动平均算法的计算公式为:

$$  
M_t^{(1)}=\frac{1}{N}\left(y_t+y_{t-1}+\cdots+y_{t-N+1}\right)  
$$

Copilot 的实现:

```python
def moving_average(data, window_size):
    window = np.ones(int(window_size))/float(window_size)
    return np.convolve(data, window, 'same')
```

实际上是使用卷积来进行处理.

![](_assets/image-20231021154901143.png)

----

移动平均法的增量形式:

$$
\begin{aligned}
M_t^{(1)}&=\frac{1}{N}\left(y_t+y_{t-1}+\cdots+y_{t-N+1}\right) \\
& =\frac{1}{N}\left(y_{t-1}+\cdots+y_{t-N}\right)+\frac{1}{N}\left(y_t-y_{t-N}\right)\\
&=M_{t-1}^{(1)}+\frac{1}{N}\left(y_t-y_{t-N}\right)
\end{aligned}
$$

#### 指数平滑法

指数平滑法的核心思想: 用历史数据的线性组合预测下一时间点的值, 而线性组合系数随距离变远而按负指数 (几何级数)衰减:

$$
\begin{align*}
\hat{x}_h(1) &\approx w x_h+w^2 x_{h-1}+\cdots\\
&=\sum_{j=1}^{\infty} w^j x_{h+1-j}
\end{align*}
$$

其中 $0<w<1$, $w$ 越小，距离远的历史观测对预测的贡献越小。

因为是加权平均，所以所有加权的和应该等于零, 由于

$$
\sum_{j=1}^{\infty} w^j=\frac{w}{1-w}
$$

则第 $j$ 个权重应为

$$
\frac{w^j}{\frac{w}{1-w}}=(1-w) w^{j-1}, j=1,2, \ldots
$$

故最终公式为:

$$
\begin{align*}
\hat{x}_h(1)&=(1-w)\left(x_h+w x_{h-1}+w^2 x_{h-2}+\ldots\right)\\
&=(1-w) \sum_{j=0}^{\infty} w^j x_{h-j}
\end{align*}
$$

一般算法用 $\alpha=1-w$ 表示, 则

$$
\begin{align*}
\hat{x}_h(1)=\alpha \sum_{j=0}^{\infty} (1-\alpha)^j x_{h-j}
\end{align*}
$$

增量形式:

由于

$$
\hat{x}_h(1)=\alpha x_h+\alpha \sum_{j=1}^{\infty}(1-\alpha)^j x_{h-j}
$$
且

$$
\begin{align*}
\alpha \sum_{j=1}^{\infty}(1-\alpha)^j x_{h-j}&=\alpha(1-\alpha) \sum_{j=0}^{\infty}(1-\alpha)^j x_{h-1-j}\\
&=(1-\alpha) \hat{x}_{h-1}(1)
\end{align*}
$$

故可得增量形式为:

$$
\hat{x}_h(1)=\alpha x_h+(1-\alpha) \hat{x}_{h-1}(1)
$$

#### 时空自回归滑动平均求和模型 STARIMA

![](_assets/image-20231021145223586.png)

![](_assets/image-20231021145306421.png)

![](_assets/image-20231021145316817.png)

![](_assets/image-20231021145855125.png)

![](_assets/image-20231021145905798.png)

#### 多变量自适应回归样条模型 MARS

![](_assets/image-20231021150132636.png)

![](_assets/image-20231021150139686.png)

![](_assets/image-20231021150209814.png)

![](_assets/image-20231021150217053.png)



# 系统分析



## 主成分分析

### 引言

![](_assets/image-20231225013619672.png)

![](_assets/image-20231225013626326.png)

![](_assets/image-20231225013631534.png)

![](_assets/image-20231225013638182.png)

![](_assets/image-20231225013645725.png)

![](_assets/image-20231225013654525.png)

### 基本原理

将坐标做平移和旋转变换.

![](_assets/image-20231225013725377.png)

![](_assets/image-20231225013736696.png)

基本准则: 分类变量的分散程度越大越有利.

![](_assets/image-20231225013754462.png)

![](_assets/image-20231225013759697.png)

![](_assets/image-20231225013833561.png)

![](_assets/image-20231225013853916.png)

![](_assets/image-20231225014333505.png)

![](_assets/image-20231225014349388.png)

![](_assets/image-20231225014353919.png)

![](_assets/image-20231225014401994.png)


### 一般情况
![](_assets/image-20231225014421317.png)

![](_assets/image-20231225014426629.png)

![](_assets/image-20231225014433676.png)

![](_assets/image-20231225014439748.png)

![](_assets/image-20231225014445257.png)

![](_assets/image-20231225014451937.png)

![](_assets/image-20231225014455839.png)

![](_assets/image-20231225014501926.png)

![](_assets/image-20231225014505672.png)

![](_assets/image-20231225014513228.png)

![](_assets/image-20231225014518938.png)

### 计算方法

![](_assets/image-20231225014535140.png)

![](_assets/image-20231225014540875.png)

### 数据压缩

![](_assets/image-20231225014545265.png)

![](_assets/image-20231225014550615.png)

![](_assets/image-20231225014556028.png)

![](_assets/image-20231225014603626.png)

![](_assets/image-20231225014646400.png)

![](_assets/image-20231225014653177.png)

![](_assets/image-20231225014657341.png)

![](_assets/image-20231225014702066.png)

![](_assets/image-20231225014707281.png)

![](_assets/image-20231225014711743.png)

![](_assets/image-20231225014716715.png)

![](_assets/image-20231225014722609.png)

![](_assets/image-20231225014727326.png)

![](_assets/image-20231225014731281.png)

![](_assets/image-20231225014736944.png)

![](_assets/image-20231225014742318.png)

![](_assets/image-20231225014747863.png)

![](_assets/image-20231225014754028.png)

![](_assets/image-20231225014759201.png)

![](_assets/image-20231225014805044.png)

![](_assets/image-20231225014809681.png)

![](_assets/image-20231225014814975.png)

注意:
- 能够利用主成分有效压缩数据, 是因为数据本身具有可压缩性
- 换句话说, 就是样本相关矩阵的特征根相差很大, 其本质是变量间近视线性相关!

![](_assets/image-20231225014939734.png)

![](_assets/image-20231225014945672.png)

![](_assets/image-20231225014950725.png)

![](_assets/image-20231225015000620.png)

### 基于 PCA 的数据压缩实例

![](_assets/image-20231225015026134.png)

![](_assets/image-20231225015030544.png)

![](_assets/image-20231225015038331.png)

![](_assets/image-20231225015042469.png)

![](_assets/image-20231225015047829.png)

![](_assets/image-20231225015053809.png)

![](_assets/image-20231225015059315.png)

### 线性回归

![](_assets/image-20231225015115218.png)

![](_assets/image-20231225015119885.png)

![](_assets/image-20231225015124540.png)

![](_assets/image-20231225015130351.png)

![](_assets/image-20231225015138142.png)

![](_assets/image-20231225015143701.png)

![](_assets/image-20231225015148848.png)

![](_assets/image-20231225015154295.png)

![](_assets/image-20231225015200051.png)



## 聚类分析


### 聚类问题的一般描述

聚类问题实际上是将包含若干元素的集合，按照某种测度，划分成若干子类。

测度是指定义在每个类上的函数, 我们的目标就是使其达到最大或最小.

聚类问题的本质是划分问题: 属于 NP 难问题.

![](_assets/image-20231225021248191.png)

![](_assets/image-20231225021253350.png)

![](_assets/image-20231225021257836.png)

![](_assets/image-20231225021304190.png)

![](_assets/image-20231225021318037.png)

![](_assets/image-20231225021357532.png)


### K-均值聚类方法

![](_assets/image-20231225021425098.png)

![](_assets/image-20231225021444855.png)

![](_assets/image-20231225021449217.png)

![](_assets/image-20231225021503490.png)

![](_assets/image-20231225021510522.png)

![](_assets/image-20231225021514536.png)

![](_assets/image-20231225021519860.png)

![](_assets/image-20231225021528897.png)

![](_assets/image-20231225021536990.png)

![](_assets/image-20231225021542938.png)

优点: 算法简单, 快速, 易于实现; 聚类结果容易解释, 适用于高维数据的聚类; 实验发现, 当各个类的分布近似为高斯分布时, 效果较好.

不足: K 均值为贪婪策略, 可能陷入局部最优, 大规模数据集上求解效率低; 对离群点和噪声点敏感; 不同初始点选取可能会导致不同的聚类结果; K 值选择较为困难; 不适于发现非凸形状或者大小差别很大的聚类

使初始的聚类中心点相互之间的距离尽可能远！

![](_assets/image-20231225021949160.png)

![](_assets/image-20231225021954332.png)

![](_assets/image-20231225022025862.png)

### 密度聚类

![](_assets/image-20231225022036974.png)

![](_assets/image-20231225022041164.png)

![](_assets/image-20231225022045020.png)

![](_assets/image-20231225022050507.png)

![](_assets/image-20231225022055246.png)

![](_assets/image-20231225022100656.png)

![](_assets/image-20231225022110276.png)

![](_assets/image-20231225022113969.png)

![](_assets/image-20231225022118057.png)

![](_assets/image-20231225022121868.png)

![](_assets/image-20231225022125503.png)

![](_assets/image-20231225022129413.png)

![](_assets/image-20231225022132987.png)

![](_assets/image-20231225022136621.png)

![](_assets/image-20231225022140688.png)

![](_assets/image-20231225022146096.png)

![](_assets/image-20231225022151952.png)

DBSCAN

优点: 可以对任意形状的稠密数据集进行聚类; 可以在聚类的同时发现噪音点; 不需要事先指定类别数目; 聚类结果不依赖节点的遍历顺序.

不足: 数据集过大时收敛时间长; 聚类质量依赖于距离公式的选取; 密度差异较大时, 超参 $\epsilon$, MinPots 选取较为困难.

----

![](_assets/image-20231225022320679.png)

![](_assets/image-20231225022326224.png)

![](_assets/image-20231225022330497.png)

![](_assets/image-20231225022336716.png)

![](_assets/image-20231225022352734.png)

![](_assets/image-20231225022356480.png)

![](_assets/image-20231225022400612.png)

![](_assets/image-20231225022406991.png)


### 系统聚类方法

![](_assets/image-20231225022416433.png)

基本步骤: 1) 首先将每个变量视为一类, 得到 $n$ 类变量 2) 每次选择最相关的两个类合并, 顺序得到 $n-1, n-2, n-3, \cdots$ 直至一类变量 3) 记录合并过程生成聚类谱系图 4) 设定阈值, 根据聚类谱系图决定最终分类

![](_assets/image-20231225022600402.png)

![](_assets/image-20231225022608055.png)

![](_assets/image-20231225022616849.png)

![](_assets/image-20231225022627770.png)

![](_assets/image-20231225022640062.png)

![](_assets/image-20231225022648122.png)

![](_assets/image-20231225022655020.png)

![](_assets/image-20231225022658970.png)

![](_assets/image-20231225022704036.png)

![](_assets/image-20231225022707566.png)

![](_assets/image-20231225022712924.png)








### 动态聚类方法

![](_assets/image-20231225022722281.png)

![](_assets/image-20231225022727740.png)

![](_assets/image-20231225022736904.png)

![](_assets/image-20231225024426262.png)

![](_assets/image-20231225024432602.png)

![](_assets/image-20231225024436581.png)

![](_assets/image-20231225024440747.png)

![](_assets/image-20231225024443920.png)

![](_assets/image-20231225024447391.png)

![](_assets/image-20231225024452787.png)









### 基于自组织映射 (SOM)的聚类方法


![](_assets/image-20231225024500620.png)

![](_assets/image-20231225024504456.png)

![](_assets/image-20231225024509954.png)

![](_assets/image-20231225024514778.png)

![](_assets/image-20231225024518991.png)

![](_assets/image-20231225024522736.png)

![](_assets/image-20231225024529664.png)

![](_assets/image-20231225024533212.png)

![](_assets/image-20231225024537488.png)




### 应用实例

![](_assets/image-20231225024543191.png)

![](_assets/image-20231225024548161.png)

![](_assets/image-20231225024553637.png)

![](_assets/image-20231225024558045.png)

![](_assets/image-20231225024602701.png)

![](_assets/image-20231225024607168.png)

![](_assets/image-20231225024611180.png)

![](_assets/image-20231225024616982.png)

![](_assets/image-20231225024620534.png)

![](_assets/image-20231225024626602.png)

![](_assets/image-20231225024631484.png)

![](_assets/image-20231225024637239.png)

![](_assets/image-20231225024645266.png)

![](_assets/image-20231225024650447.png)

![](_assets/image-20231225024656268.png)

![](_assets/image-20231225024702536.png)

![](_assets/image-20231225024707915.png)

![](_assets/image-20231225024715360.png)

![](_assets/image-20231225024720780.png)

![](_assets/image-20231225024726361.png)

![](_assets/image-20231225024731752.png)

![](_assets/image-20231225024738718.png)

![](_assets/image-20231225024742653.png)

![](_assets/image-20231225024750763.png)

![](_assets/image-20231225024755727.png)

![](_assets/image-20231225024800483.png)

![](_assets/image-20231225024804904.png)

![](_assets/image-20231225024809259.png)

![](_assets/image-20231225024813844.png)

![](_assets/image-20231225024818478.png)

![](_assets/image-20231225024824564.png)

![](_assets/image-20231225024829048.png)

![](_assets/image-20231225024833773.png)

![](_assets/image-20231225024838948.png)




## 因子分析


![](_assets/image-20231225030345922.png)

![](_assets/image-20231225030351281.png)

![](_assets/image-20231225030400258.png)

![](_assets/image-20231225030404947.png)

![](_assets/image-20231225030408877.png)

![](_assets/image-20231225030413442.png)

![](_assets/image-20231225030417541.png)

![](_assets/image-20231225030421590.png)

![](_assets/image-20231225030425216.png)

![](_assets/image-20231225030434038.png)

![](_assets/image-20231225030438680.png)

![](_assets/image-20231225030443879.png)

![](_assets/image-20231225030448867.png)

![](_assets/image-20231225030451921.png)

![](_assets/image-20231225030456048.png)

![](_assets/image-20231225030501499.png)

![](_assets/image-20231225030505605.png)

![](_assets/image-20231225030510160.png)

![](_assets/image-20231225030514041.png)

![](_assets/image-20231225030518981.png)

![](_assets/image-20231225030523648.png)

![](_assets/image-20231225030527565.png)

![](_assets/image-20231225030531881.png)

![](_assets/image-20231225030535411.png)

![](_assets/image-20231225030540734.png)

![](_assets/image-20231225030544798.png)

### 基于主成分的因子分析

![](_assets/image-20231225030559439.png)

![](_assets/image-20231225030605580.png)

![](_assets/image-20231225030609808.png)

![](_assets/image-20231225030614172.png)

![](_assets/image-20231225030618663.png)

![](_assets/image-20231225030622737.png)

![](_assets/image-20231225030628514.png)

![](_assets/image-20231225030636581.png)

### 因子正交旋转

![](_assets/image-20231225030651255.png)

![](_assets/image-20231225030657175.png)

![](_assets/image-20231225030701747.png)

![](_assets/image-20231225030705646.png)

![](_assets/image-20231225030709339.png)

![](_assets/image-20231225030713879.png)

![](_assets/image-20231225030717448.png)

![](_assets/image-20231225030721513.png)

![](_assets/image-20231225030726367.png)

![](_assets/image-20231225030730916.png)

![](_assets/image-20231225030734005.png)

![](_assets/image-20231225030737505.png)





# 系统决策

## 决策分析方法导论

20世纪70年代提出了“决策分析系统 (DSS)”概念.

决策的过程: 1) 情报: 收集信息; 2)设计: 形成候选方案 3)抉择: 根据不同标准排序, 寻优 4) 实施: 观察决策效果, 收集反馈信息

西蒙: 管理就是决策.

若存在公认的最好方案, 此时不需要决策. 但如果面临多目标, 不确定因素的情况, 方案好坏因人而异, 此时才需要决策.

决策分析就是对特定的备选项进行的系统评估. 决策分析是一种标准的方法, 而不是一种描述方法. 它展示了为最大限度的达成目标, 决策者如何运用一系列制定决策的逻辑规则.

决策是分配资源的过程, 结果是决策的效果. 如果决策在制定时是最佳选择, 这个决策就是一个好决策; 如果决策者达到了预期目标, 我们说他得到了理想的结果.

![](_assets/image-20231223112007623.png)

- 决策分析的基本依据: 决策者对不同决策后果的主观偏好
	- 决策者对所有可能的决策后果存在
	- 合理的主观偏好

决策分析的重点和难点: 如何有效地获取决策者的主观偏好.

决策环境: 以方案选择为主要内容的决策过程也随环境不同而有很大差别, 决策环境处于完全可以预测和极难预测两种情况之间.

决策环境可以归纳为三种类型:
- 确定型: 未来环境完全可以预测
	- 然而包含多个目标: 多目标决策问题
- 风险型: 未来环境有几种可能的状态和相应的后果, 可以观测每种状态和后果出现的概率
	- 概率已知: 风险型决策问题
- 不确定型: 未来环境完全不可预测
	- 概率未知: 不确定型决策问题

![](_assets/image-20231223112419739.png)

### 风险性决策分析

#### 期望值法

将采取的行动方案看成是离散的随机变量，则 ${m}$ 个方案就有 $m$ 个离散随机变量，离散变量所取之值就是行动方案相对应的益损值。

离散随机变量 $X$ 的数学期望为

$$
E(X)=\sum_{i=1}^m p_i x_i
$$

若采用决策目标（准则）是期望收益最大, 则选择收益期望值最大的行动方案为最优方案.

![](_assets/image-20231223112749887.png)

![](_assets/image-20231223112800126.png)

#### 决策树法

利用树形图模型来描述决策分析问题, 并直接在决策树图上进行决策分析.

决策树法求解步骤:
1. 绘制决策树
2. 计算各行动方案的益损期望值
3. 将计算所得的各种行动方案的益损期望值加以比较, 选择其中最大的期望值对应的方案为最优方案

![](_assets/image-20231223113025608.png)

![](_assets/image-20231223113047314.png)

----

**多级决策树**

有些决策问题需要经过多次决策才告完成. 应用决策树法进行多级决策分析叫做多级决策树.

![](_assets/image-20231223113142548.png)

![](_assets/image-20231223113209141.png)

![](_assets/image-20231223113235106.png)

### 强化学习

![](_assets/image-20231223113715561.png)

![](_assets/image-20231223113723551.png)

![](_assets/image-20231223113731623.png)

![](_assets/image-20231223113739519.png)

![](_assets/image-20231223113749126.png)

![](_assets/image-20231223113808438.png)

![](_assets/image-20231223113816717.png)

![](_assets/image-20231223113825811.png)

![](_assets/image-20231223113841601.png)

![](_assets/image-20231223113846459.png)

![](_assets/image-20231223113857378.png)

![](_assets/image-20231223113912041.png)

![](_assets/image-20231223113917124.png)

![](_assets/image-20231223113922239.png)

![](_assets/image-20231223113929177.png)

![](_assets/image-20231223113952343.png)

![](_assets/image-20231223113959369.png)

### 灵敏度分析

对状态概率的估计往往不一定十分准确. 当状态概率发生变化时, 会对所选最优方案产生怎样的影响? 或者当状态概率在何范围内变化时, 所选最优方案不变?

对以上问题的分析就是灵敏度分析.

![](_assets/image-20231223114514982.png)

![](_assets/image-20231223114521898.png)

### 情报的价值和贝叶斯决策

决策所需情报的种类.

1. 完全情报, 即完全可以肯定某一状态发生的情报
2. 非完全情报 (或称抽样情报), 即不能完全肯定某一状态发生的情报

**完全情报的价值**

有了完全情报, 决策者就可以准确预料即将出现什么状态, 即风险型决策变为确定型决策. 我们需要计算因获得该情报而使决策者的期望收益提高的数额, 并与获得情报所支付的费用相比较. 如果前者大于后者, 则获取该情报有利.

![](_assets/image-20231223114934387.png)

![](_assets/image-20231223114958161.png)

---


**非完全情报和贝叶斯决策**

在决策分析过程中, 如果得不到完全情报, 或者采集完全情报所花代价太大, 则可以采用非完全情报作为补充信息对原来的状态概率进行修正.

原来的状态概率称为: 先验概率.

修正后的状态概率称为: 后验概率.

贝叶斯决策就是用来估计由于获得了非完全情报而提高决策的效果 (即情报的价值)的方法.

贝叶斯公式:

$$
p\left(\theta_i \mid B\right)=\frac{p\left(\theta_i\right) p\left(B \mid \theta_i\right)}{\sum_{j=1}^n p\left(\theta_j\right) p\left(B \mid \theta_j\right)}
$$

![](_assets/image-20231223115651609.png)

![](_assets/image-20231223115658382.png)

![](_assets/image-20231223115727973.png)

![](_assets/image-20231223115734889.png)

![](_assets/image-20231223115745428.png)

### 决策支持系统 DSS


DSS 是在计算机用于管理的过程中产生的. DSS 是一种能够帮助决策者利用数据和模型, 解决半结构化的以计算机为基础的交互作用系统.

西蒙 (1960)年提出: 决策问题分为结构化和半结构化两大类.

结构化问题是指在决策过程开始前能够准确识别, 可用计算机实现全部自动化求解的问题. 对于结构化问题, 管理者只关心决策的效率.

半结构化问题至今没有统一定义. Stabell 于1979年提出了半结构化问题的特征: 1) 目标不明确且为非操作的, 或目标可操作, 但目标多且相互矛盾; 2) 事后难于确定决策效益变化的原因, 事前也难预测决策者采取措施对于决策效益的影响; 3) 决策者采取什么措施会影响决策效益是不确定的.

对于半结构化问题, 管理者关心决策的效能 (首先保证决策合理, 然后寻求提高效率).

![](_assets/image-20231223120557720.png)

DSS 问题管理的信息结构包括：决策任务、决策问题、求解方案、求解结果、决策结果和实施结果等.

DSS 数据管理一般采用关系型数据模型

DSS 模型管理包括结构模型、关系模型和基于知识表示模型的管理

DSS 知识管理包括知识获取、修改、删除、求精和一致性检验等

DSS 文本包括政策法规文件、决策案例等.文本管理则依赖关键词索引、分类号索引、语义网络索引等索引技术.

### 冲突分析

冲突分析是研究冲突现象的数学理论和方法, 运用数学模型来描述冲突现象. 冲突分析是决策论的一个分支.

冲突模型的基本要素:
1. 决策人 (局中人): 具有独立决策权的参与者
2. 行动: 可供局中人选择的动作
3. 策略: 一组可行的完整行动方案; 策略空间: 所有策略的集合.
4. 结局: 当所有局中人都选择了某一个策略后, 冲突所形成的结果.

几个重要概念:
1. 有限对策与无限对策: 在冲突模型中, 如果所有局中人的策略是有限的, 则该冲突问题称为有限对策, 否则称之为无限对策.
2. 稳定解: 能被所有局中人接受的冲突结局.
3. 稳定性分析: 找稳定解的过程.

![](_assets/image-20231223121853268.png)

![](_assets/image-20231223121900040.png)

![](_assets/image-20231223121926439.png)

>每种结局中有两个 1, 对应一种策略选取的方式.

![](_assets/image-20231223122020938.png)

![](_assets/image-20231223122053141.png)

![](_assets/image-20231223122356647.png)

![](_assets/image-20231223131836340.png)

![](_assets/image-20231223131846392.png)

![](_assets/image-20231223131925818.png)

![](_assets/image-20231223131938638.png)

![](_assets/image-20231223132130682.png)

![](_assets/image-20231223132136466.png)


## 不确定性决策分析



### 风险决策问题

>[风险决策的相关理论 - 知乎](https://zhuanlan.zhihu.com/p/106343637)

风险决策 (概率已知)
- 风险决策问题的数学描述
- 展望的概念及表达方法
- 合理的偏好向量应满足的条件
- 效用函数及其求法
- 决策者类型的判断方法
- 有限理性及实例

决策方案集  $A=\left\{a_1, a_2 \cdots a_n\right\}$

不确定状态集 $S=\left\{s_1, s_2 \cdots s_m\right\}$

给定决策方案后不同状态发生的概率 $\hat{p}(s \mid a), s \in S, a \in A$

决策后果 $g(s \mid a), s \in S, a \in A$

决策后果集

$$
\begin{aligned}
C & =\{g(s \mid a), s \in S, a \in A\} \\
& =\left\{c_1, c_2, \cdots, c_k\right\}, k \leq n m
\end{aligned}
$$

要确定决策者最满意的决策方案.

![](_assets/image-20231223132953141.png)

![](_assets/image-20231223132959613.png)

![](_assets/image-20231223133017728.png)

不同决策方案所产生的差异仅在于所有后果发生的概率不一样.

对于风险决策问题, 决策者的偏好本质上是对不同的概率向量 $p=\left[\begin{array}{llll}p_1 & p_2 \cdots & p_k\end{array}\right]^T$ 的偏好, 其中 $p_i$ 是后果 $i$ 出现的概率

$$
p_i \geq 0, \forall i, \quad \sum_{i=1}^k p_i=1
$$

我们将这种向量称为展望 (Prospect), 将所有展望组成的集合称为展望集. 风险决策分析的关键就是如何确定决策者对展望集中不同元素的偏好.

解决风险决策分析问题的基本思路:

设法在展望集上定义一个实函数 (效用函数) $u(p), \forall p \in P$

![](_assets/image-20231223133933837.png)

合理的偏好应该满足哪些条件:
1. 连通性: 展望集中的任意两个元素之间存在明确的优劣关系
2. 传递性
3. 单调性 (复合传递性)
4. 连续性 (有限优越性)

![](_assets/image-20231223134049232.png)

![](_assets/image-20231223134053847.png)

![](_assets/image-20231223134057353.png)

![](_assets/image-20231223134103559.png)

关于有限优越性的争论

![](_assets/image-20231223134126605.png)

![](_assets/image-20231223134140796.png)

![](_assets/image-20231223134148530.png)

![](_assets/image-20231223134157743.png)

![](_assets/image-20231223134206313.png)


#### 效用函数

结论: 如果决策者对风险决策问题的偏好满足以上四条假定, 则一定存在具有以下性质的效用函数 $\hat{u}(p), \forall p \in P$
1. 一致性
2. 线性
3. 正线性变换下的唯一性

![](_assets/image-20231223134354000.png)

![](_assets/image-20231223134402297.png)

如何确定上述效用函数?

关键步骤: 利用效用函数的线性性质进行递推.

对任意一组展望 $p(i) \in P, 1 \leq i \leq \tau$ 和实数 $\beta_i \geq 0,1 \leq i \leq \tau$, 如果 $\sum_{i=1}^\tau \beta_i=1$ 一定成立

$$
\hat{u}\left(\sum_{i=1}^\tau \beta_i p(i)\right)=\sum_{i=1}^\tau \beta_i \hat{u}(p(i))
$$

![](_assets/image-20231223135138918.png)

![](_assets/image-20231223135144956.png)

因为 $p_i \geq 0,1 \leq i \leq k$, $\sum_{i=1}^k p_i=1$, 可得

$$
\hat{u}(p)=\hat{u}\left(\sum_{i=1}^k p_i e(i)\right)=\sum_{i=1}^k p_i \hat{u}(e(i))
$$

说明: 确定效用函数的任务可以简化为确定以概率1发生每个后果的展望的效用值的问题.

![](_assets/image-20231223135301612.png)

![](_assets/image-20231223135309019.png)

![](_assets/image-20231223135316821.png)

![](_assets/image-20231223135348775.png)

> $c_3$ 是赚 400

![](_assets/image-20231223135716290.png)

![](_assets/image-20231223135800628.png)

#### 效用函数与期望收益的区别

![](_assets/image-20231223135852585.png)

一般情况下不会根据期望效益最大决策.

![](_assets/image-20231223135911567.png)


#### 决策者对风险的态度

![](_assets/image-20231223141139326.png)

- 如果某个决策者认为抽奖比以概率 1 获得均值 (0)好, 称其为冒险型决策者
- 如果某个决策者认为抽奖比以概率 1 获得均值 (0)坏, 称其为保守型决策者
- 如果某个决策者认为抽奖和以概率1获得均值(0)一样, 称其为中立型决策者

![](_assets/image-20231223142751349.png)

![](_assets/image-20231223142759378.png)

![](_assets/image-20231223142806519.png)

![](_assets/image-20231223142833661.png)

![](_assets/image-20231223142930515.png)

风险态度本质上反映了在不同基础上对增加单位收入的感受!
- 保守: 对增加单位收入的满意程度递减
- 中立: 对增加单位收入的满意程度不变
- 冒险: 对增加单位收入的满意程度递增

![](_assets/image-20231223142954158.png)


一个疑问: 直观上, 增加单位收入的满意程度会递增的 (冒险型)决策者不应该存在. 现实中抽奖活动的平均所得通常要小于买抽奖机会的钱, 但买抽奖机会的人很多.

一种解释: 效用理论的连续性 (有限优越性)假设有问题!

![](_assets/image-20231223143316058.png)

![](_assets/image-20231223143551626.png)

![](_assets/image-20231223144232942.png)

![](_assets/image-20231223144244962.png)

![](_assets/image-20231223144252099.png)

![](_assets/image-20231223144258107.png)

![](_assets/image-20231223144304121.png)

![](_assets/image-20231223144312249.png)

含义: 对于所考虑的抽奖问题, 如果卖者和买者对钱的确定性效用都是前面给出的保守型效用函数, 那么买方价格一定小于卖方价格, 所以不会成交! 

本质原因在于买卖双方的工作点不一样.

![](_assets/image-20231223144753515.png)

![](_assets/image-20231223144801070.png)

![](_assets/image-20231223144808233.png)

![](_assets/image-20231223144816332.png)

![](_assets/image-20231223144831850.png)

---

**例题**

![](_assets/image-20231223144917820.png)

![](_assets/image-20231223145228022.png)

![](_assets/image-20231223145307612.png)

![](_assets/image-20231223145334331.png)

![](_assets/image-20231223145342778.png)

![](_assets/image-20231223145410096.png)

>也就是比较函数曲线是凹的还是凸的


#### 有限理性原则

Simon, 1947

有限理性的原因: 人的知识不完备; 预测的困难性; 可能的行动方案不完全; 人的时间、注意力和计算能力有限; 人的价值取向和多元化目标不总是一致等等.

有限理性的具体表现: 搜集信息时有选择性, 对相同的信息会有不同解释; 设计方案时通常不是试图找出所有可能方案, 而是试图寻找满意方案; 抉择方案时通常不是始终寻找最优方案, 而是中止于获得满意方案.

![](_assets/image-20231223145953095.png)

![](_assets/image-20231223150011445.png)

![](_assets/image-20231223150017399.png)

![](_assets/image-20231223150023449.png)

![](_assets/image-20231223150038382.png)

![](_assets/image-20231223150043973.png)

决策问题的构建效应 (Framing Effect): 本质相同的决策问题由于表达方式不同会导致不同的决策.

前景理论: 人在面临获利时不愿冒风险; 而在面临损失时, 人人都成了冒险家.

损失的痛苦比获得所带来的喜悦更敏感, 而损失和获利是相对于参照点而言的, 改变评价事物时的参照点就会改变对风险的态度.

前景理论:
1. 大多数人在面临获利的时候是风险规避的 (确定效应)
2. 大多数人在面临损失的时候是风险喜好的 (反射效应)
3. 大多数人对得失的判断往往根据参考点决定 (参照依赖)
4. 大多数人对损失比对收益更敏感 (损失效应)

![](_assets/image-20231223150613780.png)


![](_assets/image-20231223150756456.png)

![](_assets/image-20231223150801968.png)

![](_assets/image-20231223150806184.png)

![](_assets/image-20231223150811086.png)

![](_assets/image-20231223150815640.png)

![](_assets/image-20231223150819080.png)

![](_assets/image-20231223150822360.png)

![](_assets/image-20231223150827284.png)

![](_assets/image-20231223150831449.png)

![](_assets/image-20231223150837639.png)




## 群决策分析

### 群决策问题概述

社会选择问题: 群由 $m$ 个成员组成, 决策问题有 $n$ 个方案可供选择, 每个成员对这组方案有自己的偏好顺序, 如何确定群的偏好顺序?

![](_assets/image-20231224011835816.png)

![](_assets/image-20231224011842626.png)






### 选择规则

几种常用的选择规则:
1. 简单多数 (Plurality): 每个成员选一个方案, 得票最多的获胜
2. 绝对多数 (Majority): 每个成员选一个方案, 得票选过 50%的获胜, 如果没有获胜方案, 选择得票多的重新投票
3. 加权投票: 每个成员对不同方案给出不同的分值, 总得分最多的方案获胜. 例子: Borda 规则: 每个成员对其最骗好的方案给 $n-1$ 分, 第二偏好的 $n-2$ 分, 如此类推, 总分最高的获胜.
4. 批准投票: 每个成员列出其认可的方案 (不限数目), 得到最多成员认可的方案获胜. 批准投票可以看成是一种特殊的加权投票方法, 每个成员对其认可的方案给 1 分, 不认可的给 0 分.

![](_assets/image-20231224012352716.png)

![](_assets/image-20231224012409884.png)

![](_assets/image-20231224012415257.png)

![](_assets/image-20231224012420712.png)

结论: 在群的每个成员偏好不变的情况下群的选择结果强烈地依赖于选择规则!

问题: 什么样的选择规则是合理的? 是否存在一个合理的选择规则?

#### 合理的选择规则

合理的选择规则 $\boldsymbol{\phi}: \boldsymbol{P} \mapsto \boldsymbol{O}$ 应具有的性质:
1. 公理 1: 连通性. 简单多数规则满足公理1
2. 公理 2: 传递性. 简单多数规则可能违背公理2
3. 条件 1: 完全域. 简单多数规则满足条件1
4. 条件 2: 无关方案的独立性. 简单多数规则满足条件 2
5. 条件 3: 群偏好和成员偏好的正的联系. 即: 群中成员对某些方案偏好顺序一样, 这群 (整体)对这些方案偏好顺序一样. 简单多数规则满足条件 3. Borda 规则可能违背条件3
6. 条件 4: Pareto 原则. 简单多数规则满足条件 4.
7. 条件 5: 非独裁性. 简单多数规则满足条件 5.

![](_assets/image-20231224113540210.png)

![](_assets/image-20231224113544242.png)

![](_assets/image-20231224113550772.png)

![](_assets/image-20231224113556059.png)

![](_assets/image-20231224113712817.png)

![](_assets/image-20231224113718331.png)

![](_assets/image-20231224113723752.png)

![](_assets/image-20231224113807531.png)


#### Arrow 不可能定理

Arrow 的不可能定理: 没有一个群的选择规则能够同时满足前面的两个公理和五个条件.

简单多数票规则满足公理1（ 连通性）; 条件1（ 完全域）; 条件2（无关方案的独立性）; 条件3（群偏好和成员偏好的正的联系）; 条件4（ Pareto 原则）; 条件5（ 非独裁性）. 简单多数票规则不满足公理2（ 传递性）.

若强迫其满足传递性会怎样?

![](_assets/image-20231224114034006.png)

![](_assets/image-20231224114040022.png)

![](_assets/image-20231224114044173.png)

![](_assets/image-20231224114049955.png)

一定可以设计一种偏好断面, 使得一个人构成决定性子群, 出现独裁?

![](_assets/image-20231224114131569.png)

不可能定理的根本原因: 序数效用的局限性.

![](_assets/image-20231224114151257.png)




### 策略性投票问题

![](_assets/image-20231224114159620.png)

![](_assets/image-20231224114214451.png)

![](_assets/image-20231224114218786.png)

![](_assets/image-20231224114227391.png)

![](_assets/image-20231224114237537.png)




### 几个实例

![](_assets/image-20231224114531567.png)

![](_assets/image-20231224114538124.png)

![](_assets/image-20231224114544381.png)

![](_assets/image-20231224114552888.png)

![](_assets/image-20231224114557030.png)

![](_assets/image-20231224114603420.png)

![](_assets/image-20231224114612220.png)


## 多目标决策分析

### 多目标决策问题的一般性描述

决策变量 $x=\left[\begin{array}{llll}x_1 & x_2 & \cdots & x_n\end{array}\right]$, 可行集 $S \subseteq R^n$, 目标函数 $f(x)=\left[f_1(x) f_2(x) \cdots f_m(x)\right]^T$, 寻找 $\hat{x} \in S$, 满足不存在 $x \in S$ 使得 $f(x)>f(\hat{x})$, 其中 $\succ$ 表示决策者的优先关系.

**有效解 (Pareto 解, 非劣解)**

假设每个目标都是成本型目标, 即越小越好, 如果存在 $x \in S$ 满足 $f_i(x) \leq f_i(\hat{x})$, $\forall 1 \leq i \leq m$, 并且至少有一个目标, 比如 $f_k(x)$ 满足 $f_k(x)<f_k(\hat{x})$, 那么 $\hat{x}$ 肯定不是所求决策. 上述 $\hat{x}$ 被称为劣解, 不是劣解的就叫非劣解, 有效解, 或 Pareto 解. 多目标决策实际上是在有效解集中进行决策.

![](_assets/image-20231224121106282.png)

- 有效解: 负直角锥和可行目标集只交于一点
- 弱有效解: 负直角锥内部和可行目标集不交
- 劣解: 负直角锥内部和可行目标集相交

**多目标效用函数**

如果存在函数 $u(y), y \in R^m$ 满足:

$$
\begin{gathered}
u(y(1))>u(y(2)) \Leftrightarrow y(1)>y(2) \\
u(y(1))<u(y(2)) \Leftrightarrow y(1) \prec y(2) \\
u(y(1))=u(y(2)) \Leftrightarrow y(1) \approx y(2) \\
\forall y(1), y(2) \in R^m
\end{gathered}
$$

决策问题可以变为优化问题

$$
\max _{x \in S} u(f(x))=u\left(f_1(x), \cdots, f_m(x)\right)
$$

上述 $u(y)$ 为多目标效用函数, 依赖决策者.

可根据如何获取偏好信息对不同方法分类
1. 在优化之前: 先获取所需要的偏好信息, 再进行优化; 较易实现, 但适用范围小
2. 在优化之中: 获取偏好信息和优化过程交互进行 (DSS); 较难实现, 但适用范围大
3. 在优化之后: 先优化产生足够的有效解, 再进行决策; 适用范围很小

用加权和函数近似效用函数, 求解优化问题

$$
\min _{x \in S} \sum_{i=1}^m w_i f_i(x)
$$

其中

$$
w_i>0, \forall i, \sum_{i=1}^m w_i=1
$$

可以事先一次性确定权系数, 也可以在优化过程中产生或修改.

加权和方法的优点: 简单, 物理意义明确, 一定得到有效解.

![](_assets/image-20231224121706628.png)

加权和方法的缺陷: 改变权向量, 一般情况下不能保证得到全部的有效解.

![](_assets/image-20231224121820669.png)

### 确定情况下多目标加权和方法

核心问题: 如何用权向量反映偏好?

让决策者将决策目标两两比较, 根据目标的相对重要程度估计权系数的比值.

![](_assets/image-20231224121910724.png)

再设法根据这一组比值估计权系数.

判断矩阵

$$
A=\left[\begin{array}{cccc}
1 & a_{12} & \cdots & a_{1 m} \\
a_{21} & 1 & \cdots & a_{2 m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m 1} & a_{m 2} & \cdots & a_{m m}
\end{array}\right]
$$

一致性条件

$$
a_{i j}=\frac{w_i}{w_j}=\frac{1}{\frac{w_j}{w_i}}=\frac{1}{a_{j i}}
$$

$$
a_{i k} a_{k j}=\frac{w_i}{w_k} \times \frac{w_k}{w_j}=a_{i j}
$$

![](_assets/image-20231224122138978.png)

![](_assets/image-20231224122145141.png)

![](_assets/image-20231224122150847.png)

![](_assets/image-20231224122157100.png)

![](_assets/image-20231224122204921.png)

![](_assets/image-20231224122211229.png)

![](_assets/image-20231224122217297.png)

![](_assets/image-20231224122221724.png)

![](_assets/image-20231224122227168.png)

![](_assets/image-20231224122232131.png)

![](_assets/image-20231224122251830.png)

总结: 如果判断矩阵能表示为一组权系数之比, 任意求得判断矩阵的最大特征根所对应的一个特征向量, 再将其规范化, 就可以唯一确定权向量.

![](_assets/image-20231224122340545.png)

![](_assets/image-20231224122345574.png)

![](_assets/image-20231224122349697.png)

![](_assets/image-20231224122356569.png)

总结: 如果判断矩阵的最大特征根和 $m$ 的差比较小, 就可以用其规范化的特征向量做权向量, 否则不行.

最后要解决的问题: $\lambda_{\text {max }}-m$ 大到什么程度不接受.

![](_assets/image-20231224122504524.png)

![](_assets/image-20231224122508257.png)

总结前面讨论的确定权系数的方法
1. 决策者对目标两两比较给出判断矩阵
2. 计算判断矩阵的最大特征根 $\lambda_{\text {max }}$
3. 如果 $\frac{\lambda_{\max }-m}{m-1}<0.1 C . R .(m)$, 对最大特征根的特征向量进行规范化得到权向量, 否则决策者应改进判断矩阵.


![](_assets/image-20231224122613290.png)

![](_assets/image-20231224122622937.png)

![](_assets/image-20231224122628889.png)

![](_assets/image-20231224122634156.png)

![](_assets/image-20231224122638737.png)

![](_assets/image-20231224122642462.png)

![](_assets/image-20231224122647769.png)

![](_assets/image-20231224122914087.png)

![](_assets/image-20231224122922457.png)

![](_assets/image-20231224122929117.png)

![](_assets/image-20231224122934024.png)

![](_assets/image-20231224122938959.png)

![](_assets/image-20231224123021110.png)

![](_assets/image-20231224123025196.png)

![](_assets/image-20231224123029481.png)

![](_assets/image-20231224123036713.png)

层次加权法思路总结:
1. 将总目标逐渐细化直至便于估计不同方案对最底层目标的相对优劣值
2. 借助判断矩阵估计不同方案对最底层目标的相对优劣值以及每层目标对上层目标的权向量
3. 通过逐层加权求得不同方案的总目标值

### 目的规划法

基本思想: 用目标期望值反映决策偏好, 通过逼近目标期望值获得决策者满意的方案.

原问题: $\underset{x \in S}{\operatorname{optimize}}\left[f_1(x) f_2(x) \cdots f_m(x)\right]^T$

目标期望值: $\hat{f}=\left[\begin{array}{llll}\hat{f}_1 & \hat{f}_2 & \cdots & \hat{f}_m\end{array}\right]^T$

基本模型: $\min _{x \in S} \sum_{j=1}^m w_j\left|f_j(x)-\hat{f}_j\right|$

![](_assets/image-20231224123248470.png)

![](_assets/image-20231224123252663.png)

![](_assets/image-20231224123257117.png)

![](_assets/image-20231224123305278.png)

![](_assets/image-20231224123320772.png)

![](_assets/image-20231224123326724.png)

![](_assets/image-20231224123331349.png)

![](_assets/image-20231224123334798.png)

![](_assets/image-20231224123340922.png)

![](_assets/image-20231224123346582.png)

![](_assets/image-20231224123350633.png)



### 逐步进行法

![](_assets/image-20231224123405014.png)

![](_assets/image-20231224123412834.png)

![](_assets/image-20231224123416629.png)

![](_assets/image-20231224123421656.png)





# 系统评价方法

**历史演变**

- System Analysis: 由美国 RAND 公司最早于 20 世纪 40 年代提出, 早期用于武器系统的成本和效益分析, 采用定量分析.
- 70 年代左右, 推广到更广泛的领域, 常常与制定政策相关
- 80 年代后, 特别针对信息系统建设的中系统分析方法应用广泛: 结构法, 原型法, 面向对象, 构件法.

**定义**
- 广义: 等同于系统工程
- 狭义: 通过一系列步骤, 帮助领导者选择最优方案的一种系统方法
- 是实现科学决策的重要工具

![](_assets/image-20231224131549175.png)

系统评价过程需遵循的原则:
- 内部因素与外部因素相结合
- 近期与远期利益相结合
- 局部效益与总体效益相结合
- 定性分析与定量分析相结合

![](_assets/image-20231224131641554.png)


## 层次分析法 AHP


起源: 20 世纪 70 年代由美国 Saaty 教授提出.

![](_assets/image-20231224131741236.png)

- 特点: 定性与定量分析相结合
- 适用: 不能完全用数学模型表示的多目标, 多准则, 群决策问题
- 方法: 问题分层, 因素权重分析, 方案排序, 一致性检验等整套方法.
- 应用: 80 年代初期介绍到中国, 在工程技术, 社会科学领域应用较广泛

### 问题与实例

![](_assets/image-20231224131944528.png)

![](_assets/image-20231224131954276.png)

![](_assets/image-20231224132011136.png)

![](_assets/image-20231224132026411.png)

### Saaty 提出的 AHP 方法

- Step1: 将问题按照决策要求进行层次分解, 得到决策层次 Decision Hierarchy
- Step2: 采用两两比较 pairwise comparison 方法得到各决策元素值
- Step3:  构造判断矩阵 judgments matrix 对决策元素值进行一致性检验; 若判断不一致, 返回 Step2, 重新进行两两比较; 若满足一致性, 进入 Step4
- Step4: 计算决策表的相对权重 weights
- Step5: 归一化处理相对权重值, 并得到各方案的分数值及排序情况 scores and hence rankings

![](_assets/image-20231224132751549.png)

![](_assets/image-20231224132805244.png)

![](_assets/image-20231224132827470.png)

![](_assets/image-20231224132841543.png)

![](_assets/image-20231224132850139.png)

![](_assets/image-20231224133345944.png)

若矩阵 $A=\left(a_{i j}\right)_{n \times n}$ 满足 $\forall i, j=1,2, \cdots, n, a_{i j}>0$, 则称其为正的, 如果 $\forall i, j=1,2, \cdots, n, a_{j i}=\frac{1}{a_{i j}}$, 则称其为互反的. 如果矩阵 $A=\left(a_{i j}\right)_{n \times n}$ 是正的, 互反的, 且元素以 scale $[1,9]$ 取值, 则称 $A$ 为判断矩阵.

![](_assets/image-20231224133746923.png)

**如何由判断矩阵计算出权重？**

Saaty 提出特征向量方法 **Eigenvector Method** (EM)

设 $\lambda_i(i=1,2, \cdots, n)$ 是判断矩阵 $A$ 的特征值, 即

$$
A w=\lambda_i w, w \neq 0
$$

设 $\lambda_{\max }=\max _i\left(\lambda_i\right)$ , 那么, 如下向量 $w$ 就是我们所希望的权重向量

$$
\begin{gathered}
A w=\lambda_{\max } w \\
w \in D
\end{gathered}
$$
$$
D=\left\{\left(w_1, w_2, \cdots, w_n\right)^T \mid \forall i=1,2, \cdots, n, w_i>0 \text { and } \sum_{i=1}^n w_i=1\right\}
$$

权重向量就是最大特征值对应的规范特征向量.

![](_assets/image-20231224134254464.png)

![](_assets/image-20231224134304994.png)

**“However, the validity of EM has never been fully proved.”** —— Sekitani, Yamaki(1999)

![](_assets/image-20231224134809402.png)

![](_assets/image-20231224134818761.png)

![](_assets/image-20231224134823262.png)

![](_assets/image-20231224134828919.png)

![](_assets/image-20231224134839501.png)

![](_assets/image-20231224134853193.png)

![](_assets/image-20231224134924710.png)

### 一致性检验

![](_assets/image-20231224134954266.png)

满足以下条件的矩阵 $A=\left(a_{i j}\right)_{n \times n}$ 是一致的

$$
\begin{aligned}
& \forall i, j, k=1,2, \cdots, n \\
& a_{i j}=a_{i k} a_{k j}
\end{aligned}
$$

![](_assets/image-20231224135040661.png)

**一致性度量**

定义判断矩阵 A 的一致性指标 consistency index (C.I.)如下:

$$
\text { C.I. }=\left(\lambda_{\max }-n\right) /(n-1)
$$

A 的一致性程度 consistency rate (C.R.) 定义为：

$$
\text { C.R. }=\text { C.I./ R.I. }
$$

![](_assets/image-20231224135243900.png)

![](_assets/image-20231224135253228.png)

![](_assets/image-20231224135259798.png)

![](_assets/image-20231224135306168.png)

![](_assets/image-20231224135333559.png)

![](_assets/image-20231224135338659.png)

#### AHP 方法的后续发展

![](_assets/image-20231224135359439.png)

![](_assets/image-20231224135407446.png)

![](_assets/image-20231224135412084.png)

![](_assets/image-20231224135419613.png)








