**背景**: 系统由要素构成, 要素之间存在逻辑关系, 并可以用一定的数学模型描述. 要了解系统中各要素之间的关系, 需要建立系统的结构模型. 结构模型: 使用有向连接图来描述系统各要素间的关系，以表示一个作为要素集合体的系统的模型. **结构模型**是一种几何模型。结构模型使用由节点和有向边构成的图来描述一个系统的结构。节点: 系统要素; 有向边: 要素之间的关系; 结构模型是一种以定性分析为主的模型.

**图的基本概念**: 有向连接图: 指由若干节点和有向边连接而成的图形. 其中节点的集合是 $S$, 有向边的集合是 $E$. 树: 没有回路的连通图就是树.关联树: 在节点上带有加权值 $W$, 而在边上有关联值 $r$ 的树称作关联树. 

**邻接矩阵**: 用来描述图中各节点两两之间的关系. 邻接矩阵 $A$ 的元素 $a_{i j}$ 表示为, 若 $S_i$ 与 $S_j$ 有关系则 $a_{i j}=1$, 反之为 0. 注意顺序, 有向边从 $S_i$ 到 $S_j$ 则 $a_{i j}=1$.矩阵 A 的元素全为零的行所对应的节点称为汇点，即只有有向边进入而没有离开该节点。如 S1; 矩阵 A 的元素全为零的列所对应的节点称为源点，即只有有向边离开而没有进入该节点。如 S4; 对应每一节点的行中，其元素值为 1 的数量，就是离开该节点的有向边数; 对应每一节点的列中，其元素值为1的数量，就是进入该节点的有向边数.

![image-20231225104015397](!%E7%AC%AC%E4%BA%8C%E7%AB%A0!.assets/image-20231225104015397.png)

可达矩阵: 用矩阵形式来描述有向连接图各节点之间，经过一定长度的通路后可以到达的程度. 矩阵运算: 逻辑乘取小, 逻辑加(取大).$A^2$的元素为1, 相应变量间有二次通道; 为0的话则无二次通道. 结论: $n$ 个变量的邻接矩阵 $A$，当 $k$ 大于或等于 $n$ 后， $A^k$ 的非对角线上不会有首次为 1 的元素。所以, $n$ 个变量的有向图，若两个变量间没有 $1 ， 2 ， \ldots, n-1$ 次通道，它们之间就不会有通道。所以, 研究变量间有无通道, 只需看 $A, A^2, \cdots, A^{n-1}$. 故有向图的可达矩阵: $R=I+A+A^2+\cdots+A^{n-1}$, 且由于单位矩阵运算的性质, 有$R=(I+A)^{n-1}$. 而如果有 $m<n-1$ 满足$(I+A)^m=(I+A)^{m+1}$, 则: $R=(I+A)^m$.

![image-20231225104608996](!%E7%AC%AC%E4%BA%8C%E7%AB%A0!.assets/image-20231225104608996.png)

ISM问题是由美国John Warfield教授开发的, 不能解决定量化建模问题, 一个系统可以由一个有向连接图表示,不是一种动态结构化技术. 把复杂的系统分解为若干子系统(要素)最终将系统构造成一个多级递阶的结构模型. 乒乓球, 围棋不适合用解释性结构建模方法进行排序, 跑步, 铁饼适合

ISM问题一般提法: 给定一组变量, 一组满足传递性的有向关系, 要求完全表示其相互关系的骨架图.确定骨架图的步骤: 1) 确定邻接矩阵 2)计算可达矩阵 3) 做层次划分 4) 确定骨架图. 层次划分: 若变量是“叶子节点”, 所有变量都指向它, 则是顶层变量; 如果一个变量没有指向它的变量, 则是最底层的变量. 

利用以下规则就可以确定骨架图: 1)同层变量或者互通或者不通 (根据可达矩阵判断) 2) 每层变量仅指向**相邻的**上层变量 (根据可达矩阵判断) 3) 每层变量不指向下层变量. 求骨架图也就是在反复求顶层变量. 顶层变量特征: 1) 不达到其他变量 2)如能达到某个变量, 则该变量也能达到它. 结论: 变量 $i$ 是顶层变量当且仅当其满足 $E(i) \subset F(i)$, 其中, ${E}({i})$ 表示变量 $i$ 能达到的变量的集合, ${F}({i})$ 表示能达到变量 $i$ 的变量的集合. 用邻接矩阵不能直接确定骨架图.可以用逐次求底层变量的方法构建骨架图. 错误: 在骨架图中, 最优方案一定在底层.

![image-20231225105038832](!%E7%AC%AC%E4%BA%8C%E7%AB%A0!.assets/image-20231225105038832.png)

$E(1)=\{1,2,3,5,6,8\}$, $F(1)=\{1,4,6,7\}$ 否; $E(2)=\{2,3,8\}$, $F(2)=\{1,2,3,4,6,7,8\}$ 是. 确定了 2, 3, 5, 8 是顶层变量, 所以就去掉了.然后把1, 4, 6, 7 四个变量及其可达矩阵摘出来, 发现1, 6是顶层变量, 4 ,7 不是. 最后2, 3, 5, 8是顶层, 1, 6是二层, 4是三层, 7是四层.

**确定骨架图**: 基本步骤: 1) 选择参考变量 2) 将所有变量逐个和参考变量比较 3) 考虑间接影响 4) 对所有变量分类 5) 以分析方法确定骨架图. 任务, 建立17个目标的结构模型, 变量: 17个目标项目, 关系: A不比B差(也就是有向连接), 任务: 确定项目相对优劣. 第一步, 选择项目1为参考变量, 第二步: 将其它项目和项目1 比较. 第四步: 确定对角块. 第五步: 确定非对角块: 把对角块画出来, 然后分别比较, 最终获得骨架图和可达矩阵.

![image-20231225105545564](!%E7%AC%AC%E4%BA%8C%E7%AB%A0!.assets/image-20231225105545564.png)

![image-20231225105551812](!%E7%AC%AC%E4%BA%8C%E7%AB%A0!.assets/image-20231225105551812.png)

黑箱: 不清楚物理结构, 或结构过于复杂; 不了解机理规律, 或机理过于复杂. 黑箱建模: 根据观测的输入输出数据, 寻找规律, 建立数学模型. 黑箱建模 (曲面拟合, 回归)方法: 选择由待定参数决定的一类函数 $f(x \mid \theta)$, 获取样本数据 $x(t), y(t), t=1,2, \cdots$, 拟合样本数据 $\min _\theta \sum_t(y(t)-f(x(t) \mid \theta))^2$, 获得经验模型 $y \approx f(x \mid \hat{\theta})$. 关键问题: 选择什么函数类.

黑箱建模方法的基本指标: 黑箱模型对原系统的逼近能力, 参数是否便于估计, 拟合效果如何, 预测效果如何, 模型是否好用. 模型的好用性与模型的预测效果是等价的, 模型的逼近能力与模型的拟合性是等价的.错误: 预测效果都好的模型一定是一个好的黑箱模型. 正确: 最小二乘法能使用的前提是残差白噪声. 正确:多项式模型一定具有很好的逼近效果. 正确: 黑箱建模问题的难点在于找到拟合效果和预测效果的平衡. 错误: 一般的多项式模型能满足黑箱建模的三项指标.

多项式逼近: 逼近能力: 对任意阶可导函数, 由泰勒定理保证. 对连续函数, 由魏尔斯特拉斯定理保证. 结论: 对任何连续函数, 存在可以和其任意靠近的多项式函数序列. 多项式逼近, 容易确定模型参数. 采用最小二乘方法（Least Square）估计参数 $\hat{\theta}=\left(\Phi \Phi^T\right)^{-1} \Phi Y^T$.观测值$x_1(t), x_2(t), \cdots, x_n(t), y(t)$, 逼近多项式$f(x \mid \theta)=a_0+a_1 x_1+\cdots+a_n x_n+b_{11} x_1^2+b_{12} x_1 x_2+\cdots+b_{n n} x_n^2$, 待确定参数$\theta=\left[a_0, a_1, \cdots, a_n, b_{11}, b_{12}, \cdots, b_{n n}, c_{111}, c_{112}, \cdots, c_{n n n}, \cdots\right]_{n \times 1}^T$, 向量表示 $\phi(t)=\left[1 x_1(t) \cdots x_n(t) x_1(t)^2 x_1(t) x_2(t) \cdots\right]_{n \times 1}^T$. 其中向量偏导数$\frac{\partial\left(F(\theta) G^T(\theta)\right)}{\partial \theta}=\frac{\partial F(\theta)}{\partial \theta} G^T(\theta)+F(\theta) \frac{\partial G^T(\theta)}{\partial \theta}$. 多项式逼近的本质: 只在一点研究问题, 通过不断分析该点各阶变化趋势逼近任意远处的函数值, 但是某些点的偏差可能被大幅度放大. 泰勒展开$f(x)=\sum_{n=0}^{\infty} a_n\left(x-x_0\right)^n$, $a_n=\frac{f^{(n)}\left(x_0\right)}{n !}$. 求导时行向量与列向量相合, 行行与列列不相合. 错误: 利用泰勒展开获得的多项式基函数是局部基函数.

基函数方法: 限制基函数起作用的区域, 用局部基函数代替全局基函数. 辐射基函数RBF$\rho(x \mid \beta(k))=\eta\left(\frac{x-c_k}{\delta_k}\right)$, 高斯 RBF  $\rho(x \mid \beta(k))=\exp \left(\frac{-\left\|x-c_k\right\|^2}{\delta_k^2}\right)$. 岭函数$\rho(x \mid \beta(k))=\sigma\left(w_k^T x+d_k\right)$, Sigmoid函数$\sigma(u)=\frac{1}{1+\exp (-u)}$. 两类常用人工神经网络: 高斯Radial Basis 神经网络 $\sum_k \alpha_k \exp \left(\frac{-\left\|x-c_k\right\|^2}{\delta_k^2}\right)$; Sigmoid 神经网络 $\sum_k \alpha_k \frac{1}{1+\exp \left(w_k^T x+d_k\right)}$. 逼近能力: 都能逼近任意连续函数; 模型参数: 可用基于导数的非线性规划算法, 部分参数可用最小二乘公式. 有效克服多项式函数的缺陷. 1D: 可选用hstep $(x)$ 作基函数，它只在 $I$ 区间内起作用，在其他区间不起作用，是局部基函数。辐射基函数类神经网络: 用基函数显著大于 0 的部分 $\sum_k \alpha_k \eta\left(\frac{x-c_k}{\delta_k}\right) \approx \sum_k g\left(z_k\right) h s t e p_{I_k}(x)$, 岭函数类神经网络: 用基函数接近 1 的部分, 所需要的节点数目 $M=m^n$, $\sum_k \alpha_k \sigma\left(w_k x+d_k\right)\approx g\left(z_1\right)+\sum_k\left(g\left(z_k\right)-g\left(z_{k-1}\right)\right) \sigma\left(w_k x+d_k\right)$. 错误: step函数和hstep函数是局部基函数. 正确: 证明RBFNN逼近效果是利用了其基函数与超钟型函数的相似性.

回归分析方法: 从一组数据出发, 确定因变量和自变量之间的关系式; 对关系式中的参数进行估计, 并进行假设检验; 筛选自变量, 找出影响显著的, 剔除不显著的; 用求得的回归模型做预测; 对预测结果进行分析, 评价. 线性回归问题对优化问题 $\min _{\theta \in R^n} \sum_{t=1}^N\left(y(t)-\theta^T x(t)\right)^2$, 如果存在 $\left(X X^T\right)^{-1}$, 最优解为 $\hat{\theta}=\left(X X^T\right)^{-1} X Y^T$.

**一元线性回归**: $\mathrm{y}=a+b x+\epsilon$, 则有$\hat{a}=\bar{y}-\hat{b} \bar{x}$, $\hat{b}=\frac{\sum X_i Y_i}{\sum X_i^2}=\frac{L_{x y}}{L_{x x}}$, 正则方程: $\frac{\partial\left(\sum e_i^2\right)}{\partial a}=-2 \sum\left(y_i-a-b x_i\right)=0$, $\frac{\partial\left(\sum e_i^2\right)}{\partial b}=-2 \sum x_i\left(y_i-a-b x_i\right)=0$. 回归方程检验: 相关系数分解法: 因为 $L_{y y}=\sum_{i=1}^N\left(y_i-\bar{y}\right)^2=\sum_{i=1}^N y_i^2-N \bar{y}^2=\sum_{i=1}^N\left(\hat{y}_i-\bar{y}\right)^2+\sum_{i=1}^N\left(y_i-\hat{y}_i\right)^2$, 记作总平方和 TSS = 解释平方和 ESS + 剩余平方和 RSS, 定义 $r^2=\frac{E S S}{T S S}=\frac{\sum_{i=1}^N\left(\hat{y}_i-\bar{y}\right)^2}{\sum_{i=1}^N\left(y_i-\bar{y}\right)^2}$, 表示总平方和中由回归解释了的部分, 最小二乘法将使这个部分达到最大., $r= \pm \sqrt{r^2}$ 称为相关系数, $r$ 符号与 $b$ 相同. 相关系数为0, 说明建立的一元线性回归模型无效.

也可以运用假设检验方法刻画回归方程的线性因果关系, 构造统计量 $t=\frac{r \sqrt{N-2}}{\sqrt{1-r^2}}$, 设 $r$ 是总体 $(x, y)$ 的相关系数，当假设 $H_0: r=0$ 成立时，统计量 $\mathrm{t}$ 服从自由度 (degree of freedom)为 $\mathrm{N}-2$ 的 $\mathrm{t}$ 分布. 当 $t>t_\alpha$ 时，否定原假设 (null hypothesis), 认为 $x$ 与 $y$ 存在线性关系.  F 检验法: 在假设 $H_0: b=0$ 成立时，TSS，ESS，RSS 分别是自由度为 $f_T=N-1, f_E=1, f_r=N-2$ 的 $\chi^2$ 变量，并且 RSS 与 ESS 相互独立，于是统计量 $F=\frac{E S S / f_E}{R S S / f_R}=\frac{(N-2) E S S}{R S S}$ 服从自由度为 $(1, N-2)$ 的 F 分布. 当 $F>F_\alpha$ 时，否定原假设，认为 $\mathrm{x}$ 与 $\mathrm{y}$ 存在线性关系.

精度分析: 设 $S_\delta$ 为 $\mathrm{y}$ 的剩余均方差，它表示变量 $\mathrm{y}$ 偏离回归直线的误差 $S_\sigma=\sqrt{\frac{\sum_{i=1}^N\left(y_i-\hat{y}\right)^2}{N-2}}=\sqrt{\frac{\left(1-r^2\right) L_{y y}}{N-2}}$. 给定显著性水平 $\alpha$ ，对某一 $\mathrm{x}_0$ ，相应的 $\mathrm{y}_0$ 将以 $(1-\alpha)$ 的概率落在下述区间（称为置信区间）. 式中, $\hat{y}_0$ 是对应于 $x_0$ 的 $y_0$ 的预测值， $Z_{\alpha / 2}$ 是标准正态分布上 $\alpha / 2$ 百分位点的值.

一元线性回归步骤: 数据平移和归一化压缩变换, 得到变量 $x^{\prime}=\frac{x-c_1}{d_1}, y^{\prime}=\frac{y-c_2}{d_2}$, 计算新变量的系数$\hat{b}^{\prime}=L_{x^{\prime} y^{\prime}} / L_{x^{\prime} x^{\prime}}$, $\hat{a}^{\prime}=\bar{y}^{\prime}-\hat{b}^{\prime} \bar{x}^{\prime}$, 带回原变量 $\frac{y-c_2}{d_2}=\hat{a}^{\prime}+\hat{b}^{\prime} \frac{x-c_1}{d_1}$, 进行假设检验$E S S^{\prime}=\hat{b}^{\prime 2} L_{x^{\prime} x^{\prime}}=\hat{b}^{\prime} L_{x^{\prime} y^{\prime}}$, $R S S^{\prime}=L_{y^{\prime} y^{\prime}}-E S S^{\prime}$, $F^{\prime}=\frac{(N-2) E S S}{R S S}$, 求置信区间, 对回归直线进行预测$S_\delta^{\prime}=\sqrt{\frac{R S S^{\prime}}{N-2}}$, $S_\delta=d_2 S_\delta^{\prime}$.

一元非线性回归: 函数变换线性化方法; 多项式变换线性化方法; 分段线性化方法; 直接非线形回归分析方法(最小二乘准则, 非线性规划).

多元线性回归: $y=\beta_0+\beta_1 x_1+\beta_2 x_2+\cdots+\beta_n x_n$, 定义以下向量和矩阵: $\mathbf{Y}=\left[\begin{array}{llll}y_1 & y_2 & \cdots & y_N\end{array}\right]$, $\mathbf{X}=\left[\begin{array}{cccc}1 & 1 & \cdots & 1 \\ x_{11} & x_{21} & \cdots & x_{N 1} \\ x_{12} & x_{22} & \cdots & x_{N 2} \\ \vdots & \vdots & \ddots & \vdots \\ x_{1 n} & x_{2 n} & \cdots & x_{N n}\end{array}\right]$, $\boldsymbol{\beta}=\left[\begin{array}{c}\beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_n\end{array}\right]$, $\boldsymbol{\varepsilon}=\left[\begin{array}{llll}\boldsymbol{\varepsilon}_1 & \boldsymbol{\varepsilon}_2 & \cdots & \boldsymbol{\varepsilon}_N\end{array}\right]$, 则回归方程及回归预测误差可表示为 $\mathbf{Y}=\boldsymbol{\beta}^{\mathrm{T}} \boldsymbol{X}+\boldsymbol{\varepsilon}$, 其中 $\hat{\boldsymbol{\beta}}=\left(\mathbf{X} \mathbf{X}^T\right)^{-1}\left(\mathbf{X} \mathbf{Y}^T\right)$. 又有$\mathbf{A}=\left[\begin{array}{ccccc}N & \sum_{i=1}^N x_{i 1} & \sum_{i=1}^N x_{i 2} & \cdots & \sum_{i=1}^N x_{i n} \\ \sum_{i=1}^N x_{i 1} & \sum_{i=1}^N x_{i 1}^2 & \sum_{i=1}^N x_{i 1} x_{i 2} & \cdots & \sum_{i=1}^N x_{i 1} x_{i n} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ \sum_{i=1}^N x_{i n} & \sum_{i=1}^N x_{i 1} x_{i n} & \sum_{i=1}^N x_{i 2} x_{i n} & \cdots & \sum_{i=1}^N x_{i n}^2\end{array}\right]$, $\mathbf{B}=\left[\begin{array}{c}\sum_{i=1}^N y_i \\ \sum_{i=1}^N x_{i 1} y_i \\ \vdots \\ \sum_{i=1}^N x_{i n} y_i\end{array}\right]$, 有$\mathbf{A}=\mathbf{X} \mathbf{X}^T$, $\mathbf{B}=\mathbf{X} \mathbf{Y}^T$. 

有数据预处理的方法: $y_i=\mu_0+\beta_1\left(x_{i 1}-\bar{x}_1\right)+\beta_2\left(x_{i 2}-\bar{x}_2\right)+\beta_3\left(x_{i 3}-\bar{x}_3\right)+\varepsilon_i, \quad i=1,2, \cdots, 49$, 则方程变化为 $l_{11}=\sum_{i=1}^{49} x_{i 1}^2-\frac{1}{49}\left(\sum_{i=1}^{49} x_{i 1}\right)^2$, $l_{21}=l_{12}=\sum_{i=1}^{49} x_{i 1} x_{i 2}-\frac{1}{49}\left(\sum_{i=1}^{49} x_{i 1}\right)\left(\sum_{i=1}^{49} x_{i 2}\right)$, $l_{1 y}=\sum_{i=1}^{49} x_{i 1} y_i-\frac{1}{49}\left(\sum_{i=1}^{49} x_{i 1}\right)\left(\sum_{i=1}^{49} y_i\right)$, 则$\mathbf{A}=\left[\begin{array}{ll}N & 0 \\ 0 & \mathbf{L}\end{array}\right]$可得.

显著性检验: 同样有: TSS=ESS+RSS. 在假设 $\mathrm{H}_0: \beta_1=\beta_2=\cdots=\beta_n=0$ 成立时, 统计量 $F=\frac{E S S / f_E}{R S S / f_R}=\frac{(N-n-1) \cdot E S S}{n \cdot R S S}$ 服从自由度为 $(n, N-n-1)$ 的 F 分布.当 $F>F_\alpha$ 时，否定原假设，认为 $\mathrm{x}$ 与 $\mathrm{y}$ 存在线性关系. $T S S=\sum_{i=1}^n\left(y_i-\bar{y}\right)^2$, $E S S=\sum_{i=1}^n\left(\hat{y}_i-\bar{y}\right)^2$, $R S S=\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2$. 可以用 $S_\delta=\sqrt{\frac{R S S}{N-n-1}}$, 预测值 $\hat{y}$ 将以 $(1-\alpha)$ 的概率落在下述区域内, 即 $\left(\hat{y}_0-Z_{\alpha / 2} S_\delta, \hat{y}_0+Z_{\alpha / 2} S_\delta\right)$.

病态线性回归问题. 产生原因: 样本数据中回归变量间严格线性相关! $X X^T$ 的秩不会大于 $B_m$ 的秩, 没有逆矩阵! 先求最大无关向量, 再变换得到结果. 确定满足$B_m^T B_m=I_m$, $x(t)=B_m z(t), 1 \leq t \leq N$, 的 $B_m, z(t), 1 \leq t \leq N$ 和最小的 $m$,. 由于$y(t) \approx c^T x(t)=c^T B_m z(t)=d^T z(t)$, 先估计$\hat{d}=\left(Z Z^T\right)^{-1} Z Y^T$, 再利用$z(t)=B_m^T B_m z(t)=B_m^T x(t)$. 方法: 从n递减寻找符合要求的m. 严格病态线性回归问题的处理方法: 从 $\hat{m}=n-1$ 开始逐渐减少 $\hat{m}$ ，依次求解, $\min \sum^N(x(t)-L v(t))^T(x(t)-L v(t))$, s.t. $L^T L=I_{\hat{m}}, v(t) \in R^{\hat{m}}$, 找到使最优目标值等于零的最小 $m$, 其对应的最优解就可用作所需要的$B_m$ 和 $z(t), 1 \leq t \leq N$. 对于严格病态回归问题, 可以通过多次求解优化问题来确定独立变量个数m. 错误: 上述优化问题目标函数一直远大于零, 说明对应的m还不是我们要找的m. 因为一旦 $\hat{m}<m$ ，下述优化问题的最优目标值一定大于零. 正确: 只要优化问题目标函数一直等于0, 说明对应的m还不是我们要找的m.正确: 严格病态回归问题, 估计参数的公式就不能使用了. 求解严格病态回归问题的思路是先建立Y与Z(由线性无关的变量构成)之间的回归方程,再利用Z与X的关系, 建立Y与X的方程. X=BZ, 其中Z是X在B组成的空间上的投影.

实际病态线性回归: $X X^T \approx B Z Z^T B^T$接近奇异, 其逆矩阵即使存在, 参数估计值$\hat{c}=\left(X X^T\right)^{-1} X Y^T$也很不可靠. 将实对称矩阵$X X^T$正交对角化, $X X^T$接近奇异, 本质上就是某些特征根远比其它特征根小. 类似严格病态线性回归的处理方法, 优化问题的最优目标值为$\sum_{i=m+1}^n \lambda_i$, 则处理病态线性回归问题的基本方法是找出使逼近误差$\sum_{i=m+1}^n \lambda_i$可以接受的最小正整数$m$,确定$Q_m=[q(1) q(2) \cdots q(m)]$, $Z=Q_m^T X$, $\hat{d}=\left(Z Z^T\right)^{-1} Z Y^T$, $\hat{c}=Q_m \hat{d}$, 得到$y \approx \hat{d}^T z=\hat{d}^T Q_m^T x=\hat{c}^T x$. 实际病态问题理论分析: 参数估计误差为$-\Lambda_m^{-1} Z \mu^T$, 特征根趋近于0的话倒数趋近无穷大, 线性回归误差被严重放大. 注意$\left(Z Z^T\right)^{-1}=\Lambda_m^{-1}$, $\hat{d}=\left(Z Z^T\right)^{-1} Z Y^T=\Lambda_m^{-1} Q_m^T X Y^T$. 理论: 若$A$ 为 $n$ 阶实对称矩阵, 则A的特征根皆为实数, $R^n$ 中属于 $\mathrm{A}$ 的不同特征值的特征向量必正交, 存在正交矩阵 $C$ ，使得 $C^{-1} A C$ 为对角阵. 矩阵特征值之和等于矩阵的迹. 行列式等于所有特征值的乘积. 如果n阶矩阵A的秩小于n，则A的行列式等于0.

规范化措施: 样本数据规范化$\bar{x}_i(t)=\frac{x_i(t)-e\left(x_i\right)}{\sqrt{\delta^2\left(x_i\right)}}$, 其中$e\left(x_i\right)=\frac{1}{N} \sum_{t=1}^N x_i(t)$, $\delta^2\left(x_i\right)=\frac{1}{N-1} \sum_{t=1}^N\left(x_i(t)-e\left(x_i\right)\right)^2$, 归一化的具体作用是统一样本的统计分布特性. 一般来说,线性回归不一定要做归一化, 但此时比较各回归系数就不那么方便了. 规范化: 相对误差$\frac{\sum_{t=1}^N\left(x(t)-Q_m Q_m^T x(t)\right)^T\left(x(t)-Q_m Q_m^T x(t)\right)}{\sum_{t=1}^N x^T(t) x(t)}=\frac{\sum_{i=m+1}^n \lambda_i}{\sum_{i=1}^n \lambda_i}$

交通流量预测. 对时序的研究主要基于两种方法, 一种是用随机过程的理论建立线性关系模型, 如回归模型, ARIMA模型; 另外一种方法是利用非线性动力学方法, 研究低自由度的混沌系统. 预测方法举例: 时空自回归滑动平均求和模型 STARIMA(Pfeifer在1980年提出), 多变量自适应回归样条模型 MARS(Jerome Freidman 1991). 移动平均法: $M_t^{(1)}=\frac{1}{N}\left(y_t+y_{t-1}+\cdots+y_{t-N+1}\right)$, 增量形式: $M_t^{(1)}=M_{t-1}^{(1)}+\frac{1}{N}\left(y_t-y_{t-N}\right)$, 指数平滑法$\hat{x}_h(1)=\alpha \sum_{j=0}^{\infty}(1-\alpha)^j x_{h-j}$, 增量形式$\hat{x}_h(1)=\alpha x_h+(1-\alpha) \hat{x}_{h-1}(1)$.











