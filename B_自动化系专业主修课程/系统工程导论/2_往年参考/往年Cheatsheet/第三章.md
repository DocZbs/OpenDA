**主成分分析**: 将坐标做平移和旋转变换,使得新坐标的原点与样本数据群的重心重合,第一主轴与数据变异最大的方向对应. 该问题的最优解$\hat{y}_1(t)=\hat{l}_1(1) \tilde{x}_1(t)+\hat{l}_2(1) \tilde{x}_2(t)+\hat{l}_3(1) \tilde{x}_3(t)$就是这组样本数据的第一主成分. 一般情况下, 给定一组样本数据,首先求出规格化的数据,确定m个主成分的优化模型为$\max \sum_{t=1}^N \sum_{k=1}^m\left(y_k(t)\right)^2$, s.t. $y_k(t)=\sum_{i=1}^n l_i(k) \tilde{x}_i(t), \quad k=1,2, \cdots m \leq n$, $\sum_{i=1}^n\left(l_i(k)\right)^2=1, \quad k=1,2, \cdots m$, $\sum_{i=1}^n l_i(k) l_i(j)=0, \quad \forall k \neq j$.  问题转化为$\max \sum_{k=1}^m l^T(k) \tilde{X} \tilde{X}^T l(k)$, st$L^T L=I_m$. 用$\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n \geq 0$表示$\tilde{X} \tilde{X}^T$ 的顺序递减的特征根,$q(1), q(2), \cdots, q(n)$ 是它们对应的规范化的特征向量，则所求主成分为$\hat{y}_k(t)=q^T(k) \tilde{x}(t), k=1,2, \cdots m$.**主成分样本均值**$e\left(\hat{y}_k\right)=\sum_{i=1}^n q_i(k) e\left(\tilde{x}_i\right)=0, \forall k$, 主成分样本方差$\delta^2\left(\hat{y}_k\right)=\lambda_k /(N-1), \forall k$, 主成分样本方差之和$\sum_{k=1}^n \delta^2\left(\hat{y}_k\right)=n$, 因为$(N-1) \sum_{k=1}^n \delta^2\left(\hat{y}_k\right)=\sum_{k=1}^n \sum_{t=1}^N \tilde{x}^T(t) q(k) q^T(k) \tilde{x}(t)$.

**分类变量个数选择准则**, 选择最小的 $\hat{m}$ 作为 $m$ ，满足$\frac{\sum_{k=1}^{\hat{m}} \delta^2\left(\hat{y}_k\right)}{\sum_{k=1}^n \delta^2\left(\hat{y}_k\right)}=\frac{1}{n} \sum_{k=1}^{\hat{m}} \delta^2\left(\hat{y}_k\right) \geq \gamma$.**数据压缩问题**: 给定一组样本数据,首先求出其规格化的数据,求解优化问题$\min \sum_{t=1}^N(\tilde{x}(t)-L y(t))^T(\tilde{x}(t)-L y(t))$,假定L列满秩且$L^T L=I_m$,则等价于求解$\max \sum_{k=1}^m l^T(k) \tilde{X} \tilde{X}^T l(k)$,st$L^T L=I_m$,注意下标是m,最优压缩变量是$\hat{y}_k(t)=q^T(k) \tilde{x}(t), k=1,2, \cdots m$,它就是前m个主成分. 相对逼近误差:$\frac{\sum_{t=1}^N \tilde{x}^T(t) \tilde{x}(t)-\sum_{t=1}^N \tilde{x}^T(t) \hat{L} \hat{L}^T \tilde{x}(t)}{\sum_{t=1}^N \tilde{x}^T(t) \tilde{x}(t)}=\frac{\sum_{i=m+1}^n \lambda_i}{\sum_{i=1}^n \lambda_i}=\frac{\sum_{k=m+1}^n \delta^2\left(\hat{y}_k\right)}{n}$. 主成分压缩数据所需要存储的数据个数: 假设样本数据是n维向量, 共有N个, 我们提取出了m个主成分,则需要存储数据$m \times N+m \times n+n+n$. 第一部分是在m个主成分方向上的投影, 第二部分是m个主成分向量, n是样本方差, n是样本均值. **主成分数据压缩计算过程**:对样本数据进行归一化; 计算归一化样本数据协方差矩阵$X X^T$,计算$X X^T$的特征值和特征向量; 对特征值进行排序,选取前m个特征值所对应的特征向量作为主成分方向;计算各样本数据在主成分方向上的投影$y(t)=[q(1), q(2), \cdots, q(m)]^T x(t)$,计算压缩率$\eta=\frac{m \times N+m \times n+n+n}{n \times N}$.

**聚类分析**:聚类问题实际上是将包含若干元素的集合，按照某种测度，划分成若干子类。测度是指定义在每个类上的函数, 我们的目标就是使其达到最大或最小.聚类问题的本质是划分问题: 属于 NP 难问题. **变量聚类**: r是两个变量的协方差, 希望类内协方差的值越大越好, 即类内两个变量的夹角越小越好, 类间:希望整体分类结果相关性最强最好,角度之和最小.

**K均值**:考虑向量聚类问题$\min _{\Omega} \sum_{i=1}^k \sum_{t \in \varpi_i}\left(x(t)-e_{\varpi_i}(x)\right)^T\left(x(t)-e_{\varpi_i}(x)\right)$,其中$e_{\varpi}(x)=\frac{1}{|\varpi|} \sum_{t \in \varpi} x(t)$.步骤:首先确定分类数目k, 然后在所有样本中挑选k个作为初始中心点, 逐个利用每个样本修改中心点. 对所有样本:顺序进行下述计算:将其归入与其最近的中心点所在的类, 重新计算该类的重心,并用新的重心替换中心点.**优点**: 算法简单, 快速, 易于实现; 聚类结果容易解释, 适用于高维数据的聚类; 实验发现, 当各个类的分布近似为高斯分布时, 效果较好.**不足**: K 均值为贪婪策略, 可能陷入局部最优, 大规模数据集上求解效率低; 对离群点和噪声点敏感; 不同初始点选取可能会导致不同的聚类结果; K 值选择较为困难; 不适于发现非凸形状或者大小差别很大的聚类. **改进**: Kmeans++:使初始的聚类中心点相互之间的距离尽可能远！

**基于密度的聚类方法**:DBSCAN, OPTICS, DENCLUE. DBSCAN:密度: 领域内样本点个数; 核心点:密度不小于MinPots; 边界点: 不为核心点, 但领域内有核心点;噪音点: 不为核心点也不为边界点. **步骤**:初始化参数,确定核心点集合,寻找核心点队列集合,噪音点确定.优点: 可以对任意形状的稠密数据集进行聚类; 可以在聚类的同时发现噪音点; 不需要事先指定类别数目; 聚类结果不依赖节点的遍历顺序.不足: 数据集过大时收敛时间长; 聚类质量依赖于距离公式的选取; 密度差异较大时, 超参 $\epsilon$, MinPots 选取较为困难.

**系统聚类方法**: 是一种贪婪算法. 基本步骤: 1) 首先将每个变量视为一类, 得到 $n$ 类变量 2) 每次选择最相关的两个类合并, 顺序得到 $n-1, n-2, n-3, \cdots$ 直至一类变量 3) 记录合并过程生成聚类谱系图 4) 设定阈值, 根据聚类谱系图决定最终分类. $\rho=\min R\left(x_i, x_j\right)$.

**动态聚类方法**: 考虑密度最大的几个点作为初始中心点, 把距离某个中心点最近的点划归该中心点对应的类, 用每类的重心替换中心点.如果当前的中心点和重心点都很接近,停止分类.

**因子分析**: N个同学考试, 考试科目为n, 成绩记为$x(t)=\left[x_1(t) x_2(t) \cdots x_n(t)\right]^T 1 \leq t \leq N$,若有$m<n$ 种公共能力,则考试成绩可分解为$x_i(t)=\sum_{k=1}^m a_i(k) f_k(t)+s_i(t)$,其中$A_m=[a(1) a(2) \cdots a(m)]_{n \times m}$为载荷矩阵,$x(t)=A_m f(t)+s(t)$. 样本数据规格化, 公共因子规格化, $e\left(f_k\right)=\frac{1}{N} \sum_{t=1}^N f_k(t)=0$, $\delta^2\left(f_k\right)=\frac{1}{N-1} \sum_{t=1}^N\left(f_k(t)\right)^2=1$, 因子之间线性无关. 满足条件: $1_N F^T=0$,$1_N S^T=0$,$S F^T=0$,$\frac{1}{N-1} F F^T=I_m$, $\frac{1}{N-1} S S^T=\operatorname{diag}\left\{\phi_1, \phi_2, \cdots \phi_n\right\}$. 由样本相关矩阵$R_X=A_m A_m^T+\Phi$. 可以用最小二乘估计,但是是假设白噪声.因子得分 $\hat{f}(t)=\left(A_m \Phi^{-1} A_m^T\right)^{-1} \Phi^{-1} A_m x(t)^T$.$\Phi$ 是对角矩阵. **基于主成分的因子分析**: 利用前m个主成分建立近视的m-因子模型.